{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import  confusion_matrix, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Transformers import MedicalTimeSeriesDatasetTimeGrid, MedicalTimeSeriesDatasetTuple, collate_fn, TimeSeriesGridTransformer, TimeSeriesTupleTransformer\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_type\": \"tuple\",  # Choose 'time_grid' or 'tuple'\n",
    "    \"scaler_name\": \"MinMaxScaler\", # Name of the scaler used\n",
    "\n",
    "    \"seed\": 42,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\",\n",
    "    \"epochs\": 30, # Adjust as needed\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"patience\": 7, # For early stopping based on validation performance\n",
    "\n",
    "    # Paths (MODIFY THESE)\n",
    "    \"data_dir\": \"data\", # Directory containing train/val/test data\n",
    "    \"output_dir\": \"data/model_outputs\", # Directory to save models and results\n",
    "\n",
    "    # Model Specific Hyperparameters (adjust based on model_type and tuning)\n",
    "    # Grid Transformer\n",
    "    \"grid_d_model\": 128,\n",
    "    \"grid_nhead\": 4,\n",
    "    \"grid_num_layers\": 2,\n",
    "    \"grid_dropout\": 0.2,\n",
    "    \"grid_feature_dim\": 41, # Should match your data\n",
    "\n",
    "    # Tuple Transformer\n",
    "    \"tuple_d_model\": 128,\n",
    "    \"tuple_nhead\": 8,\n",
    "    \"tuple_num_encoder_layers\": 3,\n",
    "    \"tuple_dim_feedforward\": 256, # Typically 2-4x d_model\n",
    "    \"tuple_dropout\": 0.2,\n",
    "    \"tuple_num_modalities\": 41, # 40 variables + 1 for padding (if PAD_INDEX_Z=0) Adjust if needed!\n",
    "    \"PAD_INDEX_Z\": 0, # Padding index for time and value features\n",
    "    \"tuple_modality_emb_dim\": 64,\n",
    "    \"tuple_max_seq_len\": 768, # From transformers.py or defined here\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set globally to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Add this near the top of your script ---\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility in PyTorch, NumPy, and Python.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value) # Numpy module\n",
    "    torch.manual_seed(seed_value) # PyTorch CPU seeding\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "        # Configure CuDNN for deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # Optional: Newer PyTorch versions might require this for full determinism\n",
    "        # Note: This can sometimes throw errors if a deterministic implementation isn't available\n",
    "        # try:\n",
    "        #     torch.use_deterministic_algorithms(True)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Warning: Could not enable deterministic algorithms: {e}\")\n",
    "        # Optional: Sometimes needed for deterministic matrix multiplication\n",
    "        # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    print(f\"Seed set globally to {seed_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Call this function very early in your script ---\n",
    "seed = config[\"seed\"]\n",
    "set_seed(seed_value=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(config):\n",
    "    \"\"\"Loads data and creates DataLoader objects.\"\"\"\n",
    "    \n",
    "    data_path = config[\"data_dir\"]\n",
    "\n",
    "    y_train = np.load(os.path.join(data_path, 'outcomes_sorted_set-a.npy'), allow_pickle=True)\n",
    "    y_val = np.load(os.path.join(data_path, 'outcomes_sorted_set-b.npy'),allow_pickle=True)\n",
    "    y_test = np.load(os.path.join(data_path, 'outcomes_sorted_set-c.npy'), allow_pickle=True)\n",
    "\n",
    "    if config[\"model_type\"] == \"time_grid\":\n",
    "        data_format = config[\"model_type\"]\n",
    "    elif config[\"model_type\"] == \"tuple\" or config[\"model_type\"] == \"contrast\" or config[\"model_type\"] == \"linear_probe\":\n",
    "        data_format = \"tuple\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {config['model_type']}\")\n",
    "    scaler_name = config[\"scaler_name\"]\n",
    "    X_train = np.load(os.path.join(data_path,f\"{data_format}_processed_{scaler_name}_set-a.npy\"), allow_pickle=True)\n",
    "    X_val = np.load(os.path.join(data_path,f\"{data_format}_processed_{scaler_name}_set-b.npy\"), allow_pickle=True)\n",
    "    X_test = np.load(os.path.join(data_path,f\"{data_format}_processed_{scaler_name}_set-c.npy\"), allow_pickle=True)\n",
    "\n",
    "  \n",
    "    if config[\"model_type\"] == \"time_grid\":\n",
    "      # Feature dimension check\n",
    "        config[\"grid_feature_dim\"] = X_train.shape[2] # Update based on loaded data\n",
    "        print(f\"Grid Data Shapes: Train X: {X_train.shape}, Val X: {X_val.shape}, Test X: {X_test.shape}\")\n",
    "\n",
    "        train_dataset = MedicalTimeSeriesDatasetTimeGrid(X_train, y_train)\n",
    "        val_dataset = MedicalTimeSeriesDatasetTimeGrid(X_val, y_val)\n",
    "        test_dataset = MedicalTimeSeriesDatasetTimeGrid(X_test, y_test)\n",
    "        collate_func = None # Default collate for grid data\n",
    "\n",
    "    elif config[\"model_type\"] == \"tuple\":\n",
    "      \n",
    "        # Modality count check (find max index + 1, assuming 0 is padding or valid)\n",
    "        all_z = [t[1] for patient in X_train for t in patient]\n",
    "        num_modalities = max(all_z) + 1 if all_z else 1\n",
    "        if num_modalities > config[\"tuple_num_modalities\"]:\n",
    "             print(f\"Warning: Found {num_modalities-1} as max modality index. Updating config.\")\n",
    "             config[\"tuple_num_modalities\"] = num_modalities\n",
    "        elif num_modalities < config[\"tuple_num_modalities\"]:\n",
    "             print(f\"Warning: Max modality index is {num_modalities-1}, but config expects {config['tuple_num_modalities']-1}. Using config value.\")\n",
    "\n",
    "        print(f\"Tuple Data: Train Patients: {len(X_train)}, Val Patients: {len(X_val)}, Test Patients: {len(X_test)}\")\n",
    "        print(f\"Using {config['tuple_num_modalities']} modalities (including padding index {config['PAD_INDEX_Z']}\")\n",
    "\n",
    "\n",
    "        train_dataset = MedicalTimeSeriesDatasetTuple(X_train, y_train, max_seq_len=config[\"tuple_max_seq_len\"])\n",
    "        val_dataset = MedicalTimeSeriesDatasetTuple(X_val, y_val, max_seq_len=config[\"tuple_max_seq_len\"])\n",
    "        test_dataset = MedicalTimeSeriesDatasetTuple(X_test, y_test, max_seq_len=config[\"tuple_max_seq_len\"])\n",
    "        collate_func = collate_fn # Use the custom collate_fn\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {config['model_type']}\")\n",
    "\n",
    "    print(f\"Labels - Train: {len(y_train)} (Positive: {y_train.sum()}), Val: {len(y_val)} (Positive: {y_val.sum()}), Test: {len(y_test)} (Positive: {y_test.sum()})\")\n",
    "\n",
    "    # --- Weighted Sampler for Imbalance ---\n",
    "    class_counts = np.bincount(y_train.astype(int))\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[int(t)] for t in y_train])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], sampler=sampler, collate_fn=collate_func)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"] * 2, shuffle=False, collate_fn=collate_func) # Larger batch size for validation\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"] * 2, shuffle=False, collate_fn=collate_func)\n",
    "\n",
    "    print(\"DataLoaders created.\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_model(config):\n",
    "    \"\"\"Initializes the model based on the specified type.\"\"\"\n",
    "    \n",
    "    if config[\"model_type\"] == \"time_grid\":\n",
    "        model = TimeSeriesGridTransformer(\n",
    "            feature_dim=config[\"grid_feature_dim\"],\n",
    "            d_model=config[\"grid_d_model\"],\n",
    "            nhead=config[\"grid_nhead\"],\n",
    "            num_layers=config[\"grid_num_layers\"],\n",
    "            dropout=config[\"grid_dropout\"]\n",
    "        )\n",
    "    elif config[\"model_type\"] == \"tuple\":\n",
    "        model = TimeSeriesTupleTransformer(\n",
    "            num_modalities=config[\"tuple_num_modalities\"],\n",
    "            d_model=config[\"tuple_d_model\"],\n",
    "            nhead=config[\"tuple_nhead\"],\n",
    "            num_encoder_layers=config[\"tuple_num_encoder_layers\"],\n",
    "            dim_feedforward=config[\"tuple_dim_feedforward\"],\n",
    "            num_classes=1, # Binary classification\n",
    "            dropout=config[\"tuple_dropout\"],\n",
    "            max_seq_len=config[\"tuple_max_seq_len\"],\n",
    "            modality_emb_dim=config[\"tuple_modality_emb_dim\"]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {config['model_type']}\")\n",
    "\n",
    "    model.to(config[\"device\"])\n",
    "    print(f\"Model ({config['model_type']}) created and moved to {config['device']}.\")\n",
    " \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                dataloader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                device,\n",
    "                model_type):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "   \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        \n",
    "        if model_type == \"tuple\":\n",
    "            t_seq, z_seq, v_seq, attn_mask, labels = batch\n",
    "            t_seq, z_seq, v_seq = t_seq.to(device), z_seq.to(device), v_seq.to(device)\n",
    "            attn_mask, labels = attn_mask.to(device), labels.to(device)\n",
    "            \n",
    "            logits = model(t_seq, z_seq, v_seq, attn_mask) # Pass correct mask polarity\n",
    "            y = labels # Ensure y is of shape (batch_size, 1)\n",
    "            \n",
    "        elif model_type == \"time_grid\":\n",
    "            x, label = batch\n",
    "        \n",
    "            x = x.to(device)\n",
    "            y = label.to(device)\n",
    "            logits = model(x)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)    \n",
    "   \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, model_type):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            if model_type == \"tuple\":\n",
    "                t_seq, z_seq, v_seq, attn_mask, labels = batch\n",
    "                t_seq, z_seq, v_seq, attn_mask, labels = t_seq.to(device), z_seq.to(device), v_seq.to(device), attn_mask.to(device), labels.to(device)\n",
    "                logits = model(t_seq, z_seq, v_seq, attn_mask)\n",
    "                y = labels\n",
    "\n",
    "                # Store original labels for metrics\n",
    "\n",
    "\n",
    "            elif model_type == \"time_grid\":\n",
    "                x, labels = batch\n",
    "                x = x.to(device)\n",
    "                y = labels.to(device)\n",
    "                logits = model(x)\n",
    "\n",
    "                # Store original labels for metrics\n",
    "\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "        \n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.extend(probs.flatten().flatten())\n",
    "            all_labels.extend(y.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    auroc = roc_auc_score(all_labels, all_preds)\n",
    "    auprc = average_precision_score(all_labels, all_preds)\n",
    "    confusion_matrix_result = confusion_matrix(all_labels, (all_preds > 0.5).astype(int))\n",
    "    return avg_loss, auroc, auprc, confusion_matrix_result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple Data: Train Patients: 4000, Val Patients: 4000, Test Patients: 4000\n",
      "Using 41 modalities (including padding index 0\n",
      "Labels - Train: 4000 (Positive: 554), Val: 4000 (Positive: 568), Test: 4000 (Positive: 585)\n",
      "DataLoaders created.\n",
      "Model (tuple) created and moved to cuda.\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (-1.0000 -> 0.6673). Saving model...\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.6673 -> 0.6804). Saving model...\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.6804 -> 0.6841). Saving model...\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.6841 -> 0.7030). Saving model...\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (1/7)\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7030 -> 0.7135). Saving model...\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7135 -> 0.7199). Saving model...\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (1/7)\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7199 -> 0.7308). Saving model...\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7308 -> 0.7319). Saving model...\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (1/7)\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7319 -> 0.7338). Saving model...\n",
      "\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7338 -> 0.7341). Saving model...\n",
      "\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7341 -> 0.7356). Saving model...\n",
      "\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (1/7)\n",
      "\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7356 -> 0.7396). Saving model...\n",
      "\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (1/7)\n",
      "\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (2/7)\n",
      "\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (3/7)\n",
      "\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (4/7)\n",
      "\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (5/7)\n",
      "\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (6/7)\n",
      "\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7396 -> 0.7403). Saving model...\n",
      "\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7403 -> 0.7406). Saving model...\n",
      "\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC improved (0.7406 -> 0.7406). Saving model...\n",
      "\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (1/7)\n",
      "\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (2/7)\n",
      "\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (3/7)\n",
      "\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (4/7)\n",
      "\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AuROC did not improve. (5/7)\n",
      "\n",
      "--- Loading Best Model for Testing ---\n",
      "\n",
      "--- Evaluating on Test Set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.5856\n",
      "Test AuROC: 0.7525\n",
      "Test AuPRC: 0.3277\n",
      "Confusion Matrix:\n",
      "[[2163 1252]\n",
      " [ 150  435]]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "train_loader, val_loader, test_loader = get_data_loaders(config)\n",
    "\n",
    "# 2. Model Initialization, Criterion, Optimizer\n",
    "model = get_model(config)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=config[\"patience\"])\n",
    "\n",
    "# 3. Training Loop\n",
    "best_val_auroc = -1.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = os.path.join(config[\"output_dir\"], f\"best_model_{config['model_type']}.pth\")\n",
    "\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, config[\"device\"], config[\"model_type\"])\n",
    "    val_loss, val_auroc, val_auprc, _ = evaluate(model, val_loader, criterion, config[\"device\"], config[\"model_type\"])\n",
    "\n",
    "    scheduler.step(val_auroc)\n",
    "\n",
    "    if val_auroc > best_val_auroc:\n",
    "        print(f\"Validation AuROC improved ({best_val_auroc:.4f} -> {val_auroc:.4f}). Saving model...\")\n",
    "        best_val_auroc = val_auroc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation AuROC did not improve. ({epochs_no_improve}/{config['patience']})\")\n",
    "\n",
    "    if epochs_no_improve >= config[\"patience\"]:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# 4. Load Best Model and Evaluate on Test Set\n",
    "print(\"\\n--- Loading Best Model for Testing ---\")\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=config[\"device\"]))\n",
    "\n",
    "    print(\"\\n--- Evaluating on Test Set ---\")\n",
    "    test_loss, test_auroc, test_auprc, cm = evaluate(model, test_loader, criterion, config[\"device\"], config[\"model_type\"])\n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test AuROC: {test_auroc:.4f}\")\n",
    "    print(f\"Test AuPRC: {test_auprc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(\"--------------------\")\n",
    "else:\n",
    "    print(f\"Warning: Best model file not found at {best_model_path}. Testing with the last state.\")\n",
    "    # Optionally evaluate the final model state if no best model was saved\n",
    "    print(\"\\n--- Evaluating Last Model State on Test Set ---\")\n",
    "    test_loss, test_auroc, test_auprc, cm = evaluate(model, test_loader, criterion, config[\"device\"], config[\"model_type\"])\n",
    "    print(\"\\n--- Test Results (Last Epoch Model) ---\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test AuROC: {test_auroc:.4f}\")\n",
    "    print(f\"Test AuPRC: {test_auprc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(\"--------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
