{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building up on Question 2.1.1 , adding min,max,std,skew,mean and tsfresh features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import skew, kurtosis, randint\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "static_variables = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "\n",
    "static_variables.remove('ICUType')\n",
    "\n",
    "static_variables_we_want = ['Age', 'Gender', 'Height', 'Weight']\n",
    "all_variables = ['Weight', 'Age', 'TroponinI', 'DiasABP', 'MechVent', 'HCO3', 'Cholesterol', 'HCT', 'SaO2', 'WBC', 'SysABP', 'Urine', 'ICUType', 'Gender', 'ALP', 'Creatinine', 'K', 'AST', 'Glucose', 'RespRate', 'MAP', 'FiO2', 'BUN', 'Na', 'Bilirubin', 'TroponinT', 'PaCO2', 'Height', 'GCS', 'HR', 'pH', 'PaO2', 'Lactate', 'ALT', 'NISysABP', 'RecordID', 'Platelets', 'Temp', 'Mg', 'NIDiasABP', 'Albumin', 'NIMAP']\n",
    "dyn_variables = [x for x in all_variables if x not in static_variables]\n",
    "dyn_variables.remove('ICUType')\n",
    "dyn_variables.append('Weight_VAR')\n",
    "len(dyn_variables), len(static_variables_we_want)\n",
    "\n",
    "initial_column_lists = static_variables_we_want + dyn_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import parquet file \n",
    "df_a =pd.read_parquet('data/processed_raw_data_set-a_1.parquet', engine='pyarrow')\n",
    "df_b =pd.read_parquet('data/processed_raw_data_set-b_1.parquet', engine='pyarrow')\n",
    "df_c =pd.read_parquet('data/processed_raw_data_set-c_1.parquet', engine='pyarrow')\n",
    "\n",
    "drop_ICUType = True \n",
    "if drop_ICUType:\n",
    "    df_a = df_a.drop(columns=['ICUType'])\n",
    "    df_b = df_b.drop(columns=['ICUType'])\n",
    "    df_c = df_c.drop(columns=['ICUType'])\n",
    "\n",
    "\n",
    "#  drop Time variable in df_a\n",
    "if 'Time' in df_a.columns:\n",
    "    df_a = df_a.drop(columns=['Time'])\n",
    "    df_b = df_b.drop(columns=['Time'])\n",
    "    df_c = df_c.drop(columns=['Time'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing features vectors of our patient\n",
    "\n",
    "Instead of working on the table where the missing values had already been filled, i prefer working on the not filled table because otherwise filled values would be taken into the mean and might flatten patient with lots of missing values. Then I compute the mean of variables for eahc patient over the 49 timestamps. \n",
    "\n",
    "Then i compute the median on the resulting table to fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(static_variables_we_want) , len(dyn_variables), len(static_variables_we_want) + len(dyn_variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define aggregation functions\n",
    "agg_funcs = {col: ['mean','std','max','min','skew'] for col in dyn_variables}  \n",
    "\n",
    "agg_funcs['RecordID'] = 'first'  # Keep RecordID\n",
    "for stat_var in static_variables_we_want:\n",
    "    if stat_var in df_a.columns:\n",
    "        agg_funcs[stat_var] = 'first'  # Keep static variables\n",
    "\n",
    "# Compute mean and std in one go\n",
    "df_a_agg = df_a.groupby('RecordID').agg(agg_funcs)\n",
    "\n",
    "df_a_agg.columns = ['_'.join(col).strip() for col in df_a_agg.columns.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for df_b \n",
    "agg_funcs = {col: ['mean','std','max','min','skew'] for col in dyn_variables}\n",
    "agg_funcs['RecordID'] = 'first'  # Keep RecordID\n",
    "for stat_var in static_variables_we_want:\n",
    "    if stat_var in df_b.columns:\n",
    "        agg_funcs[stat_var] = 'first'  # Keep static variables\n",
    "\n",
    "# Compute mean and std in one go\n",
    "df_b_agg = df_b.groupby('RecordID').agg(agg_funcs)\n",
    "\n",
    "df_b_agg.columns = ['_'.join(col).strip() for col in df_b_agg.columns.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for df_c\n",
    "agg_funcs = {col: ['mean','std','max','min','skew'] for col in dyn_variables}\n",
    "agg_funcs['RecordID'] = 'first'  # Keep RecordID\n",
    "for stat_var in static_variables_we_want:\n",
    "    if stat_var in df_c.columns:\n",
    "        agg_funcs[stat_var] = 'first'  # Keep static variables\n",
    "\n",
    "# Compute mean and std in one go\n",
    "df_c_agg = df_c.groupby('RecordID').agg(agg_funcs)\n",
    "df_c_agg.columns = ['_'.join(col).strip() for col in df_c_agg.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute median of df_a \n",
    "\n",
    "df_a_agg_median = df_a_agg.median()\n",
    "\n",
    "# fill missing values with median\n",
    "df_a_agg.fillna(df_a_agg_median, inplace=True)\n",
    "\n",
    "df_b_agg.fillna(df_a_agg_median, inplace=True)\n",
    "df_c_agg.fillna(df_a_agg_median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_a_agg.isnull().sum() != 0 ) print where true\n",
    "# print columns with missing values\n",
    "missing_values_a = df_a_agg.isnull().sum() != 0\n",
    "#  print only where true\n",
    "missing_values_a = missing_values_a[missing_values_a].index.tolist()\n",
    "print(\"Missing values in df_a_agg:\", missing_values_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_b = df_b_agg.isnull().sum() != 0\n",
    "#  print only where true\n",
    "missing_values_b = missing_values_b[missing_values_b].index.tolist()\n",
    "print(\"Missing values in df_b_agg:\", missing_values_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_c = df_c_agg.isnull().sum() != 0\n",
    "#  print only where true\n",
    "missing_values_c = missing_values_c[missing_values_c].index.tolist()\n",
    "print(\"Missing values in df_c_agg:\", missing_values_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Cholesterol_skew in df_a_agg, df_b_agg, df_c_agg\n",
    "df_a_agg = df_a_agg.drop(columns=['Cholesterol_skew'])\n",
    "df_b_agg = df_b_agg.drop(columns=['Cholesterol_skew'])\n",
    "df_c_agg = df_c_agg.drop(columns=['Cholesterol_skew'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_a_agg['Weight_VAR_mean'].unique()), len(df_a_agg['Weight_VAR_std'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_a_agg.isnull().sum().sum() == 0\n",
    "assert df_b_agg.isnull().sum().sum() == 0\n",
    "assert df_c_agg.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open set-a_no_nan.parquet\n",
    "df_a_no_nan = pd.read_parquet('data/set-a_no_nan.parquet', engine='pyarrow')\n",
    "df_b_no_nan = pd.read_parquet('data/set-b_no_nan.parquet', engine='pyarrow')\n",
    "df_c_no_nan = pd.read_parquet('data/set-c_no_nan.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_no_nan.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_csv('data/outcomes.csv')\n",
    "\n",
    "outcomes_a = outcomes.loc[outcomes['RecordID'].isin(df_a_no_nan['RecordID'])]\n",
    "# outcomes_a set index to RecordID\n",
    "outcomes_a = outcomes_a.set_index('RecordID')\n",
    "outcomes_a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To compute Tsfresh features on training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "we_compute_training_features = True\n",
    "\n",
    "if we_compute_training_features:\n",
    "    # 1. Load data\n",
    "    df_a_no_nan = pd.read_parquet('data/set-a_no_nan.parquet', engine='pyarrow')\n",
    "    outcomes = pd.read_csv('data/outcomes.csv')\n",
    "    outcomes_a = outcomes.loc[outcomes['RecordID'].isin(df_a_no_nan['RecordID'])]\n",
    "    outcomes_a = outcomes_a.set_index('RecordID')\n",
    "\n",
    "    # 2. Define your dynamic variables (update this list as needed)\n",
    "    # cf above\n",
    "\n",
    "    # 3. Empty list to collect features for all variables\n",
    "    all_feature_sets = []\n",
    "\n",
    "    # 4. Loop through each variable\n",
    "    for i, var in tqdm(enumerate(dyn_variables), desc=\"Processing variables\"):\n",
    "        print(f\"Processing {i}, {var}...\")\n",
    "        if (i <26):\n",
    "            continue\n",
    "        # if (i==26):\n",
    "        #     break\n",
    "        \n",
    "        df_long = df_a_no_nan[['RecordID', 'Time', var]].copy()\n",
    "        df_long = df_long.rename(columns={var: 'value'})  # tsfresh expects 'value'\n",
    "        \n",
    "        # extract tsfresh features\n",
    "        features = extract_features(df_long, column_id='RecordID', column_sort='Time', n_jobs=8)\n",
    "        \n",
    "        # drop features with NaNs\n",
    "        features = features.dropna(axis=1, how='any')\n",
    "        if features.empty:\n",
    "            continue  # skip if nothing left\n",
    "        \n",
    "        # align with labels\n",
    "        labels = outcomes_a.loc[features.index]['In-hospital_death']\n",
    "        \n",
    "        # calculate relevance\n",
    "        relevance_table = calculate_relevance_table(features, labels, ml_task='classification')\n",
    "        top_features = relevance_table[relevance_table.relevant].sort_values(\"p_value\")[\"feature\"][:5]\n",
    "        \n",
    "        # reduce to top 5 features and rename\n",
    "        selected = features[top_features]\n",
    "        selected.columns = [f\"{var}__{col}\" for col in selected.columns]\n",
    "        \n",
    "        # collect\n",
    "        all_feature_sets.append(selected)\n",
    "\n",
    "    # 5. Combine all\n",
    "\n",
    "    tsfresh_final_features = reduce(lambda left, right: left.join(right, how='outer'), all_feature_sets)\n",
    "\n",
    "    # 6. Final cleanup\n",
    "    tsfresh_final_features = tsfresh_final_features.fillna(tsfresh_final_features.median())\n",
    "\n",
    "    print(\"Final shape:\", tsfresh_final_features.shape)\n",
    "    tsfresh_final_features.head()\n",
    "\n",
    "    # save tsfresh_final_features\n",
    "    tsfresh_final_features.to_parquet('data/tsfresh_final_features_26.parquet', engine='pyarrow', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the same tsfresh features as in the traning set for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsfresh_final_features_10 = pd.read_parquet('data/tsfresh_final_features_10.parquet', engine='pyarrow') #CHANGE\n",
    "# tsfresh_final_features_10.columns\n",
    "# tsfresh_final_features_11_25 = pd.read_parquet('data/tsfresh_final_features_11_25.parquet', engine='pyarrow')\n",
    "# tsfresh_final_features_11_25.columns\n",
    "tsfresh_final_features_26 = pd.read_parquet('data/tsfresh_final_features_26.parquet', engine='pyarrow')\n",
    "tsfresh_final_features_26.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_first_double_underscore(feature_list):\n",
    "    grouped = defaultdict(list)\n",
    "    for item in feature_list:\n",
    "        if '__' in item:\n",
    "            key, rest = item.split('__', 1)  # split only at the first occurrence\n",
    "            grouped[key].append(rest)\n",
    "    return dict(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_selected = group_by_first_double_underscore(tsfresh_final_features_26.columns.tolist()) #CHANGE\n",
    "dico_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 1. Load data\n",
    "df_c_no_nan = pd.read_parquet('data/set-c_no_nan.parquet', engine='pyarrow')\n",
    "outcomes = pd.read_csv('data/outcomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_c_no_nan = df_c_no_nan.iloc[:490, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Define your dynamic variables (update this list as needed)\n",
    "# cf above\n",
    "\n",
    "# 3. Empty list to collect features for all variables\n",
    "all_feature_sets = []\n",
    "\n",
    "# 4. Loop through each variable\n",
    "for i, var in tqdm(enumerate(dyn_variables), desc=\"Processing variables\"):\n",
    "    print(f\"Processing {i}, {var}...\")\n",
    "    if var not in dico_selected.keys():\n",
    "        print(var)\n",
    "        print('not there')\n",
    "        continue\n",
    "\n",
    "    # if (i <10): \n",
    "    #     continue\n",
    "    if (i <26): \n",
    "        continue\n",
    "    # if (i==26): # change !\n",
    "    #     break\n",
    "    \n",
    "    df_long = df_c_no_nan[['RecordID', 'Time', var]].copy()\n",
    "    df_long = df_long.rename(columns={var: 'value'})  # tsfresh expects 'value'\n",
    "    \n",
    "    # extract tsfresh features\n",
    "    features = extract_features(df_long, column_id='RecordID', column_sort='Time', n_jobs=8)\n",
    "\n",
    "\n",
    "    \n",
    "    # drop features with NaNs\n",
    "    # features = features.dropna(axis=1, how='any')\n",
    "    if features.empty:\n",
    "        continue  # skip if nothing left\n",
    "    \n",
    "    top_features = dico_selected[var]\n",
    "    selected = features[top_features]\n",
    "    selected.columns = [f\"{var}__{col}\" for col in selected.columns]\n",
    "\n",
    "\n",
    "    \n",
    "    # collect\n",
    "    all_feature_sets.append(selected)\n",
    "\n",
    "# 5. Combine all\n",
    "\n",
    "tsfresh_final_features = reduce(lambda left, right: left.join(right, how='outer'), all_feature_sets)\n",
    "\n",
    "# tsfresh_final_features = tsfresh_final_features['TroponinI__value__fourier_entropy__bins_2']\n",
    "\n",
    "print(\"Final shape:\", tsfresh_final_features.shape)\n",
    "tsfresh_final_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tsfresh_final_features CHANGE\n",
    "tsfresh_final_features.to_parquet('data/tsfresh_final_features_C_10.parquet', engine='pyarrow', index=True)\n",
    "# tsfresh_final_features.to_parquet('data/tsfresh_final_features_C_11_25.parquet', engine='pyarrow', index=True)\n",
    "# tsfresh_final_features.to_parquet('data/tsfresh_final_features_C_26.parquet', engine='pyarrow', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requirements_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
