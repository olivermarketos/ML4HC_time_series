{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set globally to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Add this near the top of your script ---\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility in PyTorch, NumPy, and Python.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value) # Numpy module\n",
    "    torch.manual_seed(seed_value) # PyTorch CPU seeding\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "        # Configure CuDNN for deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # Optional: Newer PyTorch versions might require this for full determinism\n",
    "        # Note: This can sometimes throw errors if a deterministic implementation isn't available\n",
    "        # try:\n",
    "        #     torch.use_deterministic_algorithms(True)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Warning: Could not enable deterministic algorithms: {e}\")\n",
    "        # Optional: Sometimes needed for deterministic matrix multiplication\n",
    "        # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    print(f\"Seed set globally to {seed_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Call this function very early in your script ---\n",
    "SEED = 42 # Choose your desired seed value\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 4\n"
     ]
    }
   ],
   "source": [
    "static_variables = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "static_variables_we_want = ['Age', 'Gender', 'Height', 'Weight']\n",
    "all_variables = ['Weight', 'Age', 'TroponinI', 'DiasABP', 'MechVent', 'HCO3', 'Cholesterol', 'HCT', 'SaO2', 'WBC', 'SysABP', 'Urine', 'ICUType', 'Gender', 'ALP', 'Creatinine', 'K', 'AST', 'Glucose', 'RespRate', 'MAP', 'FiO2', 'BUN', 'Na', 'Bilirubin', 'TroponinT', 'PaCO2', 'Height', 'GCS', 'HR', 'pH', 'PaO2', 'Lactate', 'ALT', 'NISysABP', 'RecordID', 'Platelets', 'Temp', 'Mg', 'NIDiasABP', 'Albumin', 'NIMAP']\n",
    "dyn_variables = [x for x in all_variables if x not in static_variables]\n",
    "dyn_variables.append('Weight_VAR')\n",
    "\n",
    "print(len(dyn_variables), len(static_variables_we_want))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"data/set-a_no_nan.parquet\")\n",
    "val_df = pd.read_parquet(\"data/set-b_no_nan.parquet\")\n",
    "test_df = pd.read_parquet(\"data/set-c_no_nan.parquet\")\n",
    "\n",
    "outcomes_a_df = pd.read_csv(\"data/Outcomes-a.txt\", sep=\",\")\n",
    "outcomes_b_df = pd.read_csv(\"data/Outcomes-b.txt\", sep=\",\")\n",
    "outcomes_c_df = pd.read_csv(\"data/Outcomes-c.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,) <class 'numpy.ndarray'>\n",
      "(4000,) <class 'numpy.ndarray'>\n",
      "(4000,) <class 'numpy.ndarray'>\n",
      "All records in outcomes and data are in same order\n"
     ]
    }
   ],
   "source": [
    "tmp_outcomes = [outcomes_a_df, outcomes_b_df, outcomes_c_df]\n",
    "\n",
    "data_df = [train_df, val_df, test_df]\n",
    "\n",
    "outcome_labels = []\n",
    "for i in range(3):\n",
    "    outcome_records = tmp_outcomes[i]['RecordID'].unique().astype(int)\n",
    "    train_records = data_df[i]['RecordID'].unique().astype(int)\n",
    "    assert (outcome_records - train_records == 0).all(), \"Mismatch: expected difference of 0 between outcome_records and train_records.\"\n",
    "\n",
    "    outcome_labels.append(tmp_outcomes[i][\"In-hospital_death\"].values) # 1 if patient died in hospital, 0 otherwise\n",
    "    print(outcome_labels[i].shape, type(outcome_labels[i]))\n",
    "print(\"All records in outcomes and data are in same order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Standard scaled train data shape: (4000, 49, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert dfs into numpy arrays\n",
    "def convert_df_to_np(df):\n",
    "    dfs = []\n",
    "    for record_id in df['RecordID'].unique():\n",
    "        df_tmp = df[df['RecordID'] == record_id]\n",
    "        df_tmp = df_tmp.drop(columns=['RecordID', \"Time\"])\n",
    "        arr = df_tmp.to_numpy()\n",
    "        dfs.append(arr)\n",
    "\n",
    "    # convert list of dfs to list of tensors\n",
    "    train_data = np.array(dfs)\n",
    "    return train_data\n",
    "\n",
    "train_data = convert_df_to_np(data_df[0])\n",
    "val_data = convert_df_to_np(data_df[1])\n",
    "test_data = convert_df_to_np(data_df[2])\n",
    "\n",
    "y_train = outcome_labels[0]\n",
    "y_val = outcome_labels[1]\n",
    "y_test = outcome_labels[2]\n",
    "\n",
    "\n",
    "# Standardize data\n",
    "# Original shape: (n_patients, n_timepoints, n_features)\n",
    "n_patients, n_timepoints, n_features = train_data.shape\n",
    "\n",
    "# Reshape to 2D: (n_patients * n_timepoints, n_features)\n",
    "train_data_2d = train_data.reshape(-1, n_features)\n",
    "val_data_2d = val_data.reshape(-1, n_features)\n",
    "test_data_2d = test_data.reshape(-1, n_features)\n",
    "\n",
    "# Initialize and fit the scaler ONLY on training data\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_data_2d)\n",
    "\n",
    "# Transform all datasets\n",
    "train_scaled_2d = scaler.transform(train_data_2d)\n",
    "val_scaled_2d = scaler.transform(val_data_2d)\n",
    "test_scaled_2d = scaler.transform(test_data_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "train_data = train_scaled_2d.reshape(n_patients, n_timepoints, n_features)\n",
    "val_data = val_scaled_2d.reshape(val_data.shape)\n",
    "test_data = test_scaled_2d.reshape(test_data.shape)\n",
    "\n",
    "print(\"Sklearn Standard scaled train data shape:\", train_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MedicalTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_length=49):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, d_model) # max_length: number tokens, d_model: dimension of each token (embedding dim)\n",
    "\n",
    "        position = torch.arange(0, max_length).unsqueeze(1) # shape (max_length, 1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2)* -(torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        self.pe: torch.Tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim = 41, d_model = 128, nhead = 4, num_layers = 2,dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2) # transformer expects (seq_len, batch, features)\n",
    "        x = self.transformer_encoder(x)\n",
    "      \n",
    "        x = x[-1, :, :] # last time step\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Evaluation Functions ---\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (batch_X, batch_y) in enumerate(loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        # Gradient clipping (optional but can help stability)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print progress (optional)\n",
    "        # if (i + 1) % 50 == 0:\n",
    "        #      elapsed = time.time() - start_time\n",
    "        #      print(f'  Batch {i+1}/{len(loader)}, Loss: {loss.item():.4f}, Time: {elapsed:.2f}s')\n",
    "        #      start_time = time.time() # Reset timer\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get probabilities and predictions\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int) # Use 0.5 threshold for metrics like F1/Accuracy\n",
    "            all_preds.extend(preds.flatten())\n",
    "            all_labels.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds) # Use probabilities for AUC if needed: roc_auc_score(all_labels, probs.flatten())\n",
    "    except ValueError:\n",
    "        print(\"Warning: ROC AUC calculation failed. Likely only one class present in this evaluation batch/set.\")\n",
    "        auc = 0.0 # Or handle as appropriate\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'conf_matrix': conf_mat\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Calculated positive class weight: 6.22\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivermarketos/miniconda3/envs/mybase/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 50/125, Loss: 1.2051, Time: 1.49s\n",
      "  Batch 100/125, Loss: 1.3388, Time: 0.92s\n",
      "Epoch 1/30 | Time: 3.56s\n",
      "  Train Loss: 1.1362\n",
      "  Val Loss: 1.0967 | Val AUC: 0.6384 | Val F1: 0.3502\n",
      "  Val Accuracy: 0.7338 | Val Precision: 0.2680 | Val Recall: 0.5053\n",
      "  Val Confusion Matrix:\n",
      "[[2648  784]\n",
      " [ 281  287]]\n",
      "  * Best model saved based on Val AUC: 0.6384 at epoch 1\n",
      "  Batch 50/125, Loss: 0.9780, Time: 0.89s\n",
      "  Batch 100/125, Loss: 1.5392, Time: 0.86s\n",
      "Epoch 2/30 | Time: 2.84s\n",
      "  Train Loss: 1.0495\n",
      "  Val Loss: 1.0291 | Val AUC: 0.6956 | Val F1: 0.4137\n",
      "  Val Accuracy: 0.7512 | Val Precision: 0.3109 | Val Recall: 0.6180\n",
      "  Val Confusion Matrix:\n",
      "[[2654  778]\n",
      " [ 217  351]]\n",
      "  * Best model saved based on Val AUC: 0.6956 at epoch 2\n",
      "  Batch 50/125, Loss: 1.0178, Time: 0.89s\n",
      "  Batch 100/125, Loss: 1.2465, Time: 0.84s\n",
      "Epoch 3/30 | Time: 2.81s\n",
      "  Train Loss: 1.0074\n",
      "  Val Loss: 0.9928 | Val AUC: 0.6891 | Val F1: 0.3913\n",
      "  Val Accuracy: 0.7060 | Val Precision: 0.2771 | Val Recall: 0.6655\n",
      "  Val Confusion Matrix:\n",
      "[[2446  986]\n",
      " [ 190  378]]\n",
      "  Batch 50/125, Loss: 0.9060, Time: 0.85s\n",
      "  Batch 100/125, Loss: 1.1806, Time: 0.84s\n",
      "Epoch 4/30 | Time: 2.79s\n",
      "  Train Loss: 0.9495\n",
      "  Val Loss: 0.9648 | Val AUC: 0.7119 | Val F1: 0.4081\n",
      "  Val Accuracy: 0.6997 | Val Precision: 0.2834 | Val Recall: 0.7289\n",
      "  Val Confusion Matrix:\n",
      "[[2385 1047]\n",
      " [ 154  414]]\n",
      "  * Best model saved based on Val AUC: 0.7119 at epoch 4\n",
      "  Batch 50/125, Loss: 0.7859, Time: 0.98s\n",
      "  Batch 100/125, Loss: 1.1154, Time: 0.96s\n",
      "Epoch 5/30 | Time: 3.05s\n",
      "  Train Loss: 0.9247\n",
      "  Val Loss: 0.9554 | Val AUC: 0.7185 | Val F1: 0.4244\n",
      "  Val Accuracy: 0.7288 | Val Precision: 0.3037 | Val Recall: 0.7042\n",
      "  Val Confusion Matrix:\n",
      "[[2515  917]\n",
      " [ 168  400]]\n",
      "  * Best model saved based on Val AUC: 0.7185 at epoch 5\n",
      "  Batch 50/125, Loss: 1.3763, Time: 0.92s\n",
      "  Batch 100/125, Loss: 0.9982, Time: 0.88s\n",
      "Epoch 6/30 | Time: 2.91s\n",
      "  Train Loss: 0.9033\n",
      "  Val Loss: 0.9459 | Val AUC: 0.7271 | Val F1: 0.4423\n",
      "  Val Accuracy: 0.7522 | Val Precision: 0.3251 | Val Recall: 0.6919\n",
      "  Val Confusion Matrix:\n",
      "[[2616  816]\n",
      " [ 175  393]]\n",
      "  * Best model saved based on Val AUC: 0.7271 at epoch 6\n",
      "  Batch 50/125, Loss: 0.7429, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.3786, Time: 0.83s\n",
      "Epoch 7/30 | Time: 2.76s\n",
      "  Train Loss: 0.8802\n",
      "  Val Loss: 0.9795 | Val AUC: 0.7236 | Val F1: 0.4494\n",
      "  Val Accuracy: 0.7715 | Val Precision: 0.3416 | Val Recall: 0.6567\n",
      "  Val Confusion Matrix:\n",
      "[[2713  719]\n",
      " [ 195  373]]\n",
      "  Batch 50/125, Loss: 1.5352, Time: 0.88s\n",
      "  Batch 100/125, Loss: 0.5222, Time: 0.85s\n",
      "Epoch 8/30 | Time: 2.83s\n",
      "  Train Loss: 0.8538\n",
      "  Val Loss: 0.9686 | Val AUC: 0.7203 | Val F1: 0.4358\n",
      "  Val Accuracy: 0.7508 | Val Precision: 0.3211 | Val Recall: 0.6778\n",
      "  Val Confusion Matrix:\n",
      "[[2618  814]\n",
      " [ 183  385]]\n",
      "  Batch 50/125, Loss: 0.9422, Time: 0.89s\n",
      "  Batch 100/125, Loss: 1.2741, Time: 0.89s\n",
      "Epoch 9/30 | Time: 2.88s\n",
      "  Train Loss: 0.8265\n",
      "  Val Loss: 0.9526 | Val AUC: 0.7326 | Val F1: 0.4394\n",
      "  Val Accuracy: 0.7365 | Val Precision: 0.3148 | Val Recall: 0.7271\n",
      "  Val Confusion Matrix:\n",
      "[[2533  899]\n",
      " [ 155  413]]\n",
      "  * Best model saved based on Val AUC: 0.7326 at epoch 9\n",
      "  Batch 50/125, Loss: 1.0778, Time: 0.86s\n",
      "  Batch 100/125, Loss: 0.7186, Time: 0.86s\n",
      "Epoch 10/30 | Time: 2.79s\n",
      "  Train Loss: 0.7947\n",
      "  Val Loss: 1.0066 | Val AUC: 0.7172 | Val F1: 0.4324\n",
      "  Val Accuracy: 0.7492 | Val Precision: 0.3186 | Val Recall: 0.6725\n",
      "  Val Confusion Matrix:\n",
      "[[2615  817]\n",
      " [ 186  382]]\n",
      "  Batch 50/125, Loss: 1.0095, Time: 0.85s\n",
      "  Batch 100/125, Loss: 0.6482, Time: 0.84s\n",
      "Epoch 11/30 | Time: 2.79s\n",
      "  Train Loss: 0.7440\n",
      "  Val Loss: 1.0035 | Val AUC: 0.7202 | Val F1: 0.4501\n",
      "  Val Accuracy: 0.7782 | Val Precision: 0.3474 | Val Recall: 0.6391\n",
      "  Val Confusion Matrix:\n",
      "[[2750  682]\n",
      " [ 205  363]]\n",
      "  Batch 50/125, Loss: 0.5394, Time: 0.86s\n",
      "  Batch 100/125, Loss: 0.8963, Time: 0.84s\n",
      "Epoch 12/30 | Time: 2.79s\n",
      "  Train Loss: 0.7217\n",
      "  Val Loss: 1.0024 | Val AUC: 0.7230 | Val F1: 0.4540\n",
      "  Val Accuracy: 0.7805 | Val Precision: 0.3510 | Val Recall: 0.6426\n",
      "  Val Confusion Matrix:\n",
      "[[2757  675]\n",
      " [ 203  365]]\n",
      "  Batch 50/125, Loss: 0.3728, Time: 0.89s\n",
      "  Batch 100/125, Loss: 0.6465, Time: 0.85s\n",
      "Epoch 13/30 | Time: 2.83s\n",
      "  Train Loss: 0.7086\n",
      "  Val Loss: 0.9965 | Val AUC: 0.7267 | Val F1: 0.4516\n",
      "  Val Accuracy: 0.7705 | Val Precision: 0.3418 | Val Recall: 0.6655\n",
      "  Val Confusion Matrix:\n",
      "[[2704  728]\n",
      " [ 190  378]]\n",
      "  Batch 50/125, Loss: 0.9858, Time: 0.88s\n",
      "  Batch 100/125, Loss: 0.5908, Time: 0.87s\n",
      "Epoch 14/30 | Time: 2.87s\n",
      "  Train Loss: 0.7032\n",
      "  Val Loss: 1.0557 | Val AUC: 0.7060 | Val F1: 0.4489\n",
      "  Val Accuracy: 0.7993 | Val Precision: 0.3678 | Val Recall: 0.5757\n",
      "  Val Confusion Matrix:\n",
      "[[2870  562]\n",
      " [ 241  327]]\n",
      "  Batch 50/125, Loss: 0.7534, Time: 0.85s\n",
      "  Batch 100/125, Loss: 1.5443, Time: 0.85s\n",
      "Epoch 15/30 | Time: 2.79s\n",
      "  Train Loss: 0.6980\n",
      "  Val Loss: 1.0445 | Val AUC: 0.7107 | Val F1: 0.4508\n",
      "  Val Accuracy: 0.7947 | Val Precision: 0.3635 | Val Recall: 0.5933\n",
      "  Val Confusion Matrix:\n",
      "[[2842  590]\n",
      " [ 231  337]]\n",
      "  Batch 50/125, Loss: 0.4837, Time: 0.85s\n",
      "  Batch 100/125, Loss: 0.5598, Time: 0.85s\n",
      "Epoch 16/30 | Time: 2.78s\n",
      "  Train Loss: 0.7001\n",
      "  Val Loss: 1.0366 | Val AUC: 0.7153 | Val F1: 0.4532\n",
      "  Val Accuracy: 0.7913 | Val Precision: 0.3608 | Val Recall: 0.6092\n",
      "  Val Confusion Matrix:\n",
      "[[2819  613]\n",
      " [ 222  346]]\n",
      "  Batch 50/125, Loss: 0.7503, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.8480, Time: 0.91s\n",
      "Epoch 17/30 | Time: 2.85s\n",
      "  Train Loss: 0.6891\n",
      "  Val Loss: 1.0270 | Val AUC: 0.7187 | Val F1: 0.4531\n",
      "  Val Accuracy: 0.7857 | Val Precision: 0.3554 | Val Recall: 0.6250\n",
      "  Val Confusion Matrix:\n",
      "[[2788  644]\n",
      " [ 213  355]]\n",
      "  Batch 50/125, Loss: 0.3647, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.6106, Time: 0.84s\n",
      "Epoch 18/30 | Time: 2.83s\n",
      "  Train Loss: 0.6910\n",
      "  Val Loss: 1.0258 | Val AUC: 0.7229 | Val F1: 0.4570\n",
      "  Val Accuracy: 0.7855 | Val Precision: 0.3567 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2781  651]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.5716, Time: 0.85s\n",
      "  Batch 100/125, Loss: 0.4388, Time: 0.85s\n",
      "Epoch 19/30 | Time: 2.80s\n",
      "  Train Loss: 0.6803\n",
      "  Val Loss: 1.0254 | Val AUC: 0.7228 | Val F1: 0.4567\n",
      "  Val Accuracy: 0.7853 | Val Precision: 0.3564 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2780  652]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 1.0253, Time: 0.85s\n",
      "  Batch 100/125, Loss: 0.4310, Time: 0.85s\n",
      "Epoch 20/30 | Time: 2.82s\n",
      "  Train Loss: 0.6965\n",
      "  Val Loss: 1.0253 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.5468, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.4774, Time: 0.83s\n",
      "Epoch 21/30 | Time: 2.75s\n",
      "  Train Loss: 0.6803\n",
      "  Val Loss: 1.0253 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.2708, Time: 0.88s\n",
      "  Batch 100/125, Loss: 0.5715, Time: 0.86s\n",
      "Epoch 22/30 | Time: 2.88s\n",
      "  Train Loss: 0.6842\n",
      "  Val Loss: 1.0254 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.5894, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.5659, Time: 0.88s\n",
      "Epoch 23/30 | Time: 2.83s\n",
      "  Train Loss: 0.6918\n",
      "  Val Loss: 1.0254 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.9168, Time: 0.87s\n",
      "  Batch 100/125, Loss: 0.6585, Time: 0.85s\n",
      "Epoch 24/30 | Time: 2.81s\n",
      "  Train Loss: 0.6806\n",
      "  Val Loss: 1.0254 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.4879, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.6523, Time: 0.85s\n",
      "Epoch 25/30 | Time: 2.87s\n",
      "  Train Loss: 0.6802\n",
      "  Val Loss: 1.0255 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.5529, Time: 0.85s\n",
      "  Batch 100/125, Loss: 0.4184, Time: 0.87s\n",
      "Epoch 26/30 | Time: 2.84s\n",
      "  Train Loss: 0.6952\n",
      "  Val Loss: 1.0255 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.5163, Time: 0.84s\n",
      "  Batch 100/125, Loss: 0.4265, Time: 0.89s\n",
      "Epoch 27/30 | Time: 2.85s\n",
      "  Train Loss: 0.6817\n",
      "  Val Loss: 1.0254 | Val AUC: 0.7226 | Val F1: 0.4564\n",
      "  Val Accuracy: 0.7850 | Val Precision: 0.3560 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2779  653]\n",
      " [ 207  361]]\n",
      "  Batch 50/125, Loss: 0.2450, Time: 0.89s\n",
      "  Batch 100/125, Loss: 1.0612, Time: 0.87s\n",
      "Epoch 28/30 | Time: 2.89s\n",
      "  Train Loss: 0.6823\n",
      "  Val Loss: 1.0254 | Val AUC: 0.7216 | Val F1: 0.4551\n",
      "  Val Accuracy: 0.7845 | Val Precision: 0.3550 | Val Recall: 0.6338\n",
      "  Val Confusion Matrix:\n",
      "[[2778  654]\n",
      " [ 208  360]]\n",
      "  Batch 50/125, Loss: 0.4606, Time: 0.88s\n",
      "  Batch 100/125, Loss: 0.6321, Time: 0.85s\n",
      "Epoch 29/30 | Time: 2.82s\n",
      "  Train Loss: 0.6906\n",
      "  Val Loss: 1.0255 | Val AUC: 0.7216 | Val F1: 0.4551\n",
      "  Val Accuracy: 0.7845 | Val Precision: 0.3550 | Val Recall: 0.6338\n",
      "  Val Confusion Matrix:\n",
      "[[2778  654]\n",
      " [ 208  360]]\n",
      "  Batch 50/125, Loss: 0.4902, Time: 0.90s\n",
      "  Batch 100/125, Loss: 0.7964, Time: 0.92s\n",
      "Epoch 30/30 | Time: 2.95s\n",
      "  Train Loss: 0.6916\n",
      "  Val Loss: 1.0255 | Val AUC: 0.7225 | Val F1: 0.4561\n",
      "  Val Accuracy: 0.7847 | Val Precision: 0.3557 | Val Recall: 0.6356\n",
      "  Val Confusion Matrix:\n",
      "[[2778  654]\n",
      " [ 207  361]]\n",
      "\n",
      "--- Training Finished ---\n",
      "Best validation AUC: 0.7326 achieved at epoch 9\n",
      "\n",
      "--- Evaluating on Test Set ---\n",
      "Loaded best model weights for testing.\n",
      "Test Set Performance:\n",
      "  Test Loss: 1.1701\n",
      "  Test AUC: 0.6321\n",
      "  Test F1: 0.3226\n",
      "  Test Accuracy: 0.4783\n",
      "  Test Precision: 0.1991\n",
      "  Test Recall: 0.8496\n",
      "  Test Confusion Matrix:\n",
      "[[1416 1999]\n",
      " [  88  497]]\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters and Setup ---\n",
    "model_save_path = \"./data/V6_best_transformer_model_weighted_loss.pth\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4 # Common starting point for transformers\n",
    "WEIGHT_DECAY = 1e-5 # AdamW uses weight decay\n",
    "EPOCHS = 30 # Start with a reasonable number, monitor validation loss/AUC\n",
    "D_MODEL = 128\n",
    "N_HEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2 # Increased dropout slightly\n",
    "\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = MedicalTimeSeriesDataset(train_data, y_train)\n",
    "val_dataset = MedicalTimeSeriesDataset(val_data, y_val)\n",
    "test_dataset = MedicalTimeSeriesDataset(test_data, y_test)\n",
    "\n",
    "# --- Handle Imbalance (Method 1: Weighted Loss) ---\n",
    "# Calculate weights: weight = total_samples / (num_classes * samples_in_class)\n",
    "# Or simpler for binary: weight for positive class = num_negative / num_positive\n",
    "num_positives = y_train.sum()\n",
    "num_negatives = len(y_train) - num_positives\n",
    "pos_weight_val = num_negatives / num_positives\n",
    "pos_weight = torch.tensor([pos_weight_val], dtype= torch.float32,  device=DEVICE) # Wrap in tensor for BCEWithLogitsLoss\n",
    "print(f\"Calculated positive class weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# Loss Function with weighting\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#---------------------\n",
    "\n",
    "# --- Handle Imbalance (Method 2: Weighted Sampler - Use either this OR weighted loss, or sometimes both) ---\n",
    "# Comment out the WeightedRandomSampler lines if using only weighted loss.\n",
    "# Using both can sometimes be beneficial but start with one. Let's use WeightedRandomSampler here.\n",
    "\n",
    "# class_counts = np.bincount(y_train.astype(int))\n",
    "# class_weights = 1. / class_counts\n",
    "# sample_weights = np.array([class_weights[int(t)] for t in y_train])\n",
    "# sample_weights = torch.from_numpy(sample_weights).double()\n",
    "# sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss() # Use this if using WeightedRandomSampler\n",
    "# # Use the sampler ONLY for the training loader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "#---------------------\n",
    "\n",
    "# For validation and test, use standard sequential loading\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Model, Optimizer ---\n",
    "model = TimeSeriesTransformer(\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3)\n",
    "\n",
    "# --- Training Loop ---\n",
    "best_val_auc = -1.0 # Or use F1 score, depending on priority\n",
    "best_epoch = -1\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_f1': []}\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_epoch_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_metrics = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    val_loss = val_metrics['loss']\n",
    "    val_auc = val_metrics['auc']\n",
    "    val_f1 = val_metrics['f1']\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    end_epoch_time = time.time()\n",
    "    epoch_duration = end_epoch_time - start_epoch_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Time: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_metrics['accuracy']:.4f} | Val Precision: {val_metrics['precision']:.4f} | Val Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"  Val Confusion Matrix:\\n{val_metrics['conf_matrix']}\")\n",
    "\n",
    "\n",
    "    # Optional: Learning rate scheduling step based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the best model based on validation AUC (or F1)\n",
    "    # If using F1, ensure it's calculated correctly (might need probability adjustment/threshold tuning)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"  * Best model saved based on Val AUC: {best_val_auc:.4f} at epoch {epoch+1}\")\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f} achieved at epoch {best_epoch+1}\")\n",
    "\n",
    "# --- Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "print(\"Loaded best model weights for testing.\")\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"  Test AUC: {test_metrics['auc']:.4f}\")\n",
    "print(f\"  Test F1: {test_metrics['f1']:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Test Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"  Test Confusion Matrix:\\n{test_metrics['conf_matrix']}\")\n",
    "\n",
    "# --- Optional: Plot training history ---\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history['train_loss'], label='Train Loss')\n",
    "# plt.plot(history['val_loss'], label='Val Loss')\n",
    "# plt.title('Loss History')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history['val_auc'], label='Val AUC')\n",
    "# plt.plot(history['val_f1'], label='Val F1')\n",
    "# plt.title('Validation Metrics History')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Metric Value')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TimeSeriesTransformer(num_layers=2, nhead=4)\n",
    "\n",
    "# model_name = \"data/V5_transformer_model_2layers_4heads_128dim\" \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else  \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# n_samples = len(outcome_labels[0])\n",
    "# n_positives = sum(outcome_labels[0])\n",
    "# n_negatives = n_samples - n_positives\n",
    "\n",
    "# pos_weight_val = n_negatives / n_positives\n",
    "\n",
    "# train_dataset = MedicalTimeSeriesDataset(train_data, outcome_labels[0])\n",
    "# val_dataset = MedicalTimeSeriesDataset(val_data, outcome_labels[1])\n",
    "# test_dataset = MedicalTimeSeriesDataset(test_data, outcome_labels[2])\n",
    "\n",
    "# batch_size = 32\n",
    "# weights = np.where(outcome_labels[0] == 1, n_negatives / n_positives, 1.0)\n",
    "# sampler = WeightedRandomSampler(weights.tolist(), num_samples=n_samples, replacement=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler) # Use sampler for balanced sampling\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Calculated pos_weight: {pos_weight_val:.4f} (Negatives={n_negatives}, Positives={n_positives})\")\n",
    "# pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32).to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # Added weight decay\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# # Training function\n",
    "# def train_epoch(model, loader, criterion, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for X_batch, y_batch in loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch).squeeze()\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# # Validation/testing function\n",
    "# def evaluate(model, loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_outputs = [] # Store raw outputs (logits)\n",
    "#     all_targets = [] # Store targets\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in loader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             outputs = model(X_batch).squeeze() # raw logits\n",
    "\n",
    "#             # Ensure y_batch has the same shape as outputs if squeeze() removed a dim\n",
    "#             if outputs.dim() == 0: # Handle batch size of 1 if squeezed to scalar\n",
    "#                  outputs = outputs.unsqueeze(0)\n",
    "#             if y_batch.dim() > outputs.dim(): # Ensure y_batch has same shape as output\n",
    "#                 y_batch = y_batch.squeeze()\n",
    "#             elif outputs.dim() > y_batch.dim(): # This shouldn't happen with .squeeze() above but safety check\n",
    "#                 outputs = outputs.squeeze() # Re-try squeeze if needed\n",
    "\n",
    "#             # Ensure shapes match before loss calculation\n",
    "#             if outputs.shape != y_batch.shape:\n",
    "#                  # This indicates a potential issue elsewhere, maybe with batch size 1 handling\n",
    "#                  print(f\"Shape mismatch: outputs {outputs.shape}, y_batch {y_batch.shape}\")\n",
    "#                  # Decide how to handle: skip batch, reshape if possible, etc.\n",
    "#                  # For now, we'll just report loss can't be computed for this batch\n",
    "#                  continue # Skip this batch if shapes don't match\n",
    "\n",
    "\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             all_outputs.extend(outputs.cpu().numpy())\n",
    "#             all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "#     all_outputs = np.array(all_outputs)\n",
    "#     all_targets = np.array(all_targets)\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     avg_loss = total_loss / len(loader)\n",
    "\n",
    "#   # Apply sigmoid to logits to get probabilities for thresholding\n",
    "#     probs = 1 / (1 + np.exp(-all_outputs)) # Sigmoid function\n",
    "#     preds_labels = (probs >= 0.5).astype(int) # Threshold probabilities\n",
    "\n",
    "#     # Ensure targets are integers for accuracy score\n",
    "#     all_targets = all_targets.astype(int)\n",
    "\n",
    "#     accuracy = accuracy_score(all_targets, preds_labels)\n",
    "#     # ROC AUC can be calculated directly from logits (or probabilities)\n",
    "#     # Check if there's more than one class present in targets for ROC AUC\n",
    "#     if len(np.unique(all_targets)) > 1:\n",
    "#         roc_auc = roc_auc_score(all_targets, all_outputs) # Use logits directly\n",
    "#     else:\n",
    "#         roc_auc = 0.5 # Or np.nan, indicating AUC is not defined\n",
    "#         print(f\"Warning: Only one class present in targets. ROC AUC set to {roc_auc}\")\n",
    "\n",
    "\n",
    "#     return avg_loss, accuracy, roc_auc\n",
    "\n",
    "    \n",
    "# num_epochs = 20 # Increase epochs, rely on saving the best\n",
    "# best_val_roc_auc = 0.0 # Initialize low for maximization\n",
    "# patience_counter = 0\n",
    "# patience = 5 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "# print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "#     val_loss, val_accuracy, val_roc_auc = evaluate(model, val_loader, criterion, device)\n",
    "#     scheduler.step(val_roc_auc) # Adjust learning rate based on validation ROC AUC\n",
    "\n",
    "\n",
    "#     print(f'Epoch {epoch:02d}/{num_epochs}')\n",
    "#     print(f'  Train Loss: {train_loss:.4f}')\n",
    "#     print(f'  Val Loss: {val_loss:.4f}, Val Acc (at 0.5): {val_accuracy:.4f}, Val ROC-AUC: {val_roc_auc:.4f}') # Label accuracy as potentially misleading\n",
    "\n",
    "#     # Save best model based on validation ROC-AUC\n",
    "#     if val_roc_auc > best_val_roc_auc:\n",
    "#         print(f'  Validation ROC-AUC improved ({best_val_roc_auc:.4f} --> {val_roc_auc:.4f}). Saving model...')\n",
    "#         best_val_roc_auc = val_roc_auc # Correctly update the best AUC score\n",
    "#         torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "#         patience_counter = 0 # Reset patience counter\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         print(f'  Validation ROC-AUC did not improve. Patience {patience_counter}/{patience}')\n",
    "\n",
    "   \n",
    "# print(\"\\nTraining finished.\")\n",
    "\n",
    "# # Load best model (make sure the file exists)\n",
    "# print(f\"Loading best model saved with ROC-AUC: {best_val_roc_auc:.4f}\")\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: '{model_name}.pt' not found. Was a model ever saved?\")\n",
    "#     # Handle error appropriately - maybe exit or proceed with the last state of the model?\n",
    "\n",
    "# # Evaluate on test set\n",
    "# print('\\nEvaluating on test set with the loaded best model...')\n",
    "# test_loss, test_accuracy, test_roc_auc = evaluate(model, test_loader, criterion, device)\n",
    "# print('\\nTest set evaluation:')\n",
    "# # Remind that accuracy uses 0.5 threshold\n",
    "# print(f'Test Loss: {test_loss:.4f}, Test Acc (at 0.5): {test_accuracy:.4f}, Test ROC-AUC: {test_roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- After loading the best model ---\n",
    "# print(\"\\nFinding optimal threshold on validation set...\")\n",
    "# model.eval() # Ensure model is in eval mode\n",
    "# val_logits = []\n",
    "# val_targets_list = []\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in val_loader:\n",
    "#         X_batch = X_batch.to(device)\n",
    "#         outputs = model(X_batch).squeeze()\n",
    "#         val_logits.extend(outputs.cpu().numpy())\n",
    "#         val_targets_list.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# val_logits = np.array(val_logits)\n",
    "# val_targets_np = np.array(val_targets_list).astype(int)\n",
    "# val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "# # Method 1: Maximize F1 score\n",
    "# precision, recall, thresholds_pr = precision_recall_curve(val_targets_np, val_probs)\n",
    "# # Calculate F1 score, handling potential division by zero\n",
    "# f1_scores = 2 * recall * precision / (recall + precision + 1e-8)\n",
    "# # thresholds_pr correspond to precision/recall pairs, need to adjust index\n",
    "# optimal_idx_f1 = np.argmax(f1_scores)\n",
    "# optimal_threshold_f1 = thresholds_pr[optimal_idx_f1]\n",
    "# print(f\"Optimal threshold based on max F1 score on validation set: {optimal_threshold_f1:.4f}\")\n",
    "\n",
    "# # Method 2: Maximize Youden's J (sensitivity + specificity - 1)\n",
    "# fpr, tpr, thresholds_roc = roc_curve(val_targets_np, val_probs)\n",
    "# youden_j = tpr - fpr\n",
    "# optimal_idx_j = np.argmax(youden_j)\n",
    "# optimal_threshold_j = thresholds_roc[optimal_idx_j]\n",
    "# print(f\"Optimal threshold based on max Youden's J on validation set: {optimal_threshold_j:.4f}\")\n",
    "\n",
    "# # Choose one threshold (e.g., from F1)\n",
    "# # optimal_threshold =  optimal_threshold_j \n",
    "# # or\n",
    "# optimal_threshold =  optimal_threshold_f1 \n",
    "\n",
    "# def evaluate_with_threshold(model, loader, criterion, device, threshold):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_outputs = [] # Store raw outputs (logits)\n",
    "#     all_targets = [] # Store targets\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # (Same loop as before to get all_outputs and all_targets)\n",
    "#         for X_batch, y_batch in loader:\n",
    "#             # ... (identical data loading and model prediction) ...\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             outputs = model(X_batch).squeeze() # raw logits\n",
    "\n",
    "#             if outputs.dim() == 0: outputs = outputs.unsqueeze(0)\n",
    "#             if y_batch.dim() > outputs.dim(): y_batch = y_batch.squeeze()\n",
    "#             elif outputs.dim() > y_batch.dim(): outputs = outputs.squeeze()\n",
    "\n",
    "#             if outputs.shape != y_batch.shape:\n",
    "#                  print(f\"Shape mismatch: outputs {outputs.shape}, y_batch {y_batch.shape}\")\n",
    "#                  continue\n",
    "\n",
    "#             loss = criterion(outputs, y_batch)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             all_outputs.extend(outputs.cpu().numpy())\n",
    "#             all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "#     all_outputs = np.array(all_outputs)\n",
    "#     all_targets = np.array(all_targets)\n",
    "#     avg_loss = total_loss / len(loader)\n",
    "\n",
    "#     # Apply sigmoid and OPTIMAL threshold\n",
    "#     probs = 1 / (1 + np.exp(-all_outputs))\n",
    "#     preds_labels = (probs >= threshold).astype(int) # Use the optimal threshold\n",
    "\n",
    "#     all_targets = all_targets.astype(int)\n",
    "\n",
    "#     accuracy = accuracy_score(all_targets, preds_labels)\n",
    "#     roc_auc = roc_auc_score(all_targets, all_outputs) if len(np.unique(all_targets)) > 1 else 0.5\n",
    "\n",
    "#     # Calculate other metrics\n",
    "#     precision = precision_score(all_targets, preds_labels, zero_division=0)\n",
    "#     recall = recall_score(all_targets, preds_labels, zero_division=0)\n",
    "#     f1 = f1_score(all_targets, preds_labels, zero_division=0)\n",
    "\n",
    "#     return avg_loss, accuracy, roc_auc, precision, recall, f1\n",
    "\n",
    "\n",
    "# # Evaluate on test set using the found threshold\n",
    "# print(f'\\nEvaluating on test set using threshold: {optimal_threshold:.4f}')\n",
    "# test_loss, test_accuracy, test_roc_auc, test_precision, test_recall, test_f1 = evaluate_with_threshold(\n",
    "#     model, test_loader, criterion, device, optimal_threshold\n",
    "# )\n",
    "\n",
    "# print('\\nTest set evaluation (with optimized threshold):')\n",
    "# print(f'- Test Loss: {test_loss:.4f}')\n",
    "# print(f'- Test ROC-AUC: {test_roc_auc:.4f}')\n",
    "# print(f'- Test Accuracy: {test_accuracy:.4f}')\n",
    "# print(f'- Test Precision: {test_precision:.4f}')\n",
    "# print(f'- Test Recall: {test_recall:.4f}')\n",
    "# print(f'- Test F1-Score: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finding optimal threshold on validation set...\n",
    "\n",
    "Using standard scaling:\n",
    "- Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.9818\n",
    "- Test ROC-AUC: 0.7919\n",
    "- Test Accuracy: 0.6032\n",
    "- Test Precision: 0.2420\n",
    "- Test Recall: 0.8034\n",
    "- Test F1-Score: 0.3720\n",
    "\n",
    "Robust scaling:\n",
    "Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.8851\n",
    "- Test ROC-AUC: 0.7476\n",
    "- Test Accuracy: 0.5727\n",
    "- Test Precision: 0.2282\n",
    "- Test Recall: 0.8068\n",
    "- Test F1-Score: 0.3558\n",
    "\n",
    "Scikit robust scaling\n",
    "Evaluating on test set using threshold: 0.4240\n",
    "\n",
    "Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.8678\n",
    "- Test ROC-AUC: 0.7493\n",
    "- Test Accuracy: 0.7432\n",
    "- Test Precision: 0.3065\n",
    "- Test Recall: 0.5983\n",
    "- Test F1-Score: 0.4053\n",
    "\n",
    "Scikit standard scaling\n",
    "Optimal threshold based on max F1 score on validation set: 0.5811\n",
    "Optimal threshold based on max Youden's J on validation set: 0.4707\n",
    "\n",
    "Evaluating on test set using threshold: 0.5811\n",
    "\n",
    "Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.8664\n",
    "- Test ROC-AUC: 0.7889\n",
    "- Test Accuracy: 0.6062\n",
    "- Test Precision: 0.2446\n",
    "- Test Recall: 0.8103\n",
    "- Test F1-Score: 0.3757"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
