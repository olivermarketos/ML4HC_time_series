{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set globally to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Add this near the top of your script ---\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility in PyTorch, NumPy, and Python.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value) # Numpy module\n",
    "    torch.manual_seed(seed_value) # PyTorch CPU seeding\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "        # Configure CuDNN for deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # Optional: Newer PyTorch versions might require this for full determinism\n",
    "        # Note: This can sometimes throw errors if a deterministic implementation isn't available\n",
    "        # try:\n",
    "        #     torch.use_deterministic_algorithms(True)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Warning: Could not enable deterministic algorithms: {e}\")\n",
    "        # Optional: Sometimes needed for deterministic matrix multiplication\n",
    "        # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    print(f\"Seed set globally to {seed_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Call this function very early in your script ---\n",
    "SEED = 42 # Choose your desired seed value\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 4\n"
     ]
    }
   ],
   "source": [
    "static_variables = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "static_variables_we_want = ['Age', 'Gender', 'Height', 'Weight']\n",
    "all_variables = ['Weight', 'Age', 'TroponinI', 'DiasABP', 'MechVent', 'HCO3', 'Cholesterol', 'HCT', 'SaO2', 'WBC', 'SysABP', 'Urine', 'ICUType', 'Gender', 'ALP', 'Creatinine', 'K', 'AST', 'Glucose', 'RespRate', 'MAP', 'FiO2', 'BUN', 'Na', 'Bilirubin', 'TroponinT', 'PaCO2', 'Height', 'GCS', 'HR', 'pH', 'PaO2', 'Lactate', 'ALT', 'NISysABP', 'RecordID', 'Platelets', 'Temp', 'Mg', 'NIDiasABP', 'Albumin', 'NIMAP']\n",
    "dyn_variables = [x for x in all_variables if x not in static_variables]\n",
    "dyn_variables.append('Weight_VAR')\n",
    "\n",
    "print(len(dyn_variables), len(static_variables_we_want))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"data/set-a_no_nan.parquet\")\n",
    "val_df = pd.read_parquet(\"data/set-b_no_nan.parquet\")\n",
    "test_df = pd.read_parquet(\"data/set-c_no_nan.parquet\")\n",
    "\n",
    "outcomes_a_df = pd.read_csv(\"data/Outcomes-a.txt\", sep=\",\")\n",
    "outcomes_b_df = pd.read_csv(\"data/Outcomes-b.txt\", sep=\",\")\n",
    "outcomes_c_df = pd.read_csv(\"data/Outcomes-c.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,) <class 'numpy.ndarray'>\n",
      "(4000,) <class 'numpy.ndarray'>\n",
      "(4000,) <class 'numpy.ndarray'>\n",
      "All records in outcomes and data are in same order\n"
     ]
    }
   ],
   "source": [
    "tmp_outcomes = [outcomes_a_df, outcomes_b_df, outcomes_c_df]\n",
    "\n",
    "data_df = [train_df, val_df, test_df]\n",
    "\n",
    "outcome_labels = []\n",
    "for i in range(3):\n",
    "    outcome_records = tmp_outcomes[i]['RecordID'].unique().astype(int)\n",
    "    train_records = data_df[i]['RecordID'].unique().astype(int)\n",
    "    assert (outcome_records - train_records == 0).all(), \"Mismatch: expected difference of 0 between outcome_records and train_records.\"\n",
    "\n",
    "    outcome_labels.append(tmp_outcomes[i][\"In-hospital_death\"].values) # 1 if patient died in hospital, 0 otherwise\n",
    "    print(outcome_labels[i].shape, type(outcome_labels[i]))\n",
    "print(\"All records in outcomes and data are in same order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Standard scaled train data shape: (4000, 49, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert dfs into numpy arrays\n",
    "def convert_df_to_np(df):\n",
    "    dfs = []\n",
    "    for record_id in df['RecordID'].unique():\n",
    "        df_tmp = df[df['RecordID'] == record_id]\n",
    "        df_tmp = df_tmp.drop(columns=['RecordID', \"Time\"])\n",
    "        arr = df_tmp.to_numpy()\n",
    "        dfs.append(arr)\n",
    "\n",
    "    # convert list of dfs to list of tensors\n",
    "    train_data = np.array(dfs)\n",
    "    return train_data\n",
    "\n",
    "train_data = convert_df_to_np(data_df[0])\n",
    "val_data = convert_df_to_np(data_df[1])\n",
    "test_data = convert_df_to_np(data_df[2])\n",
    "\n",
    "# Standardize data\n",
    "# Original shape: (n_patients, n_timepoints, n_features)\n",
    "n_patients, n_timepoints, n_features = train_data.shape\n",
    "\n",
    "# Reshape to 2D: (n_patients * n_timepoints, n_features)\n",
    "train_data_2d = train_data.reshape(-1, n_features)\n",
    "val_data_2d = val_data.reshape(-1, n_features)\n",
    "test_data_2d = test_data.reshape(-1, n_features)\n",
    "\n",
    "# Initialize and fit the scaler ONLY on training data\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_data_2d)\n",
    "\n",
    "# Transform all datasets\n",
    "train_scaled_2d = scaler.transform(train_data_2d)\n",
    "val_scaled_2d = scaler.transform(val_data_2d)\n",
    "test_scaled_2d = scaler.transform(test_data_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "train_data = train_scaled_2d.reshape(n_patients, n_timepoints, n_features)\n",
    "val_data = val_scaled_2d.reshape(val_data.shape)\n",
    "test_data = test_scaled_2d.reshape(test_data.shape)\n",
    "\n",
    "print(\"Sklearn Standard scaled train data shape:\", train_data.shape)\n",
    "\n",
    "# train_data = (train_data - median) / iqr\n",
    "# val_data = (val_data - median) / iqr  \n",
    "# test_data = (test_data - median) / iqr  #\n",
    "\n",
    "# print(train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MedicalTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_length=49):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, d_model) # max_length: number tokens, d_model: dimension of each token (embedding dim)\n",
    "\n",
    "        position = torch.arange(0, max_length).unsqueeze(1) # shape (max_length, 1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2)* -(torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        self.pe: torch.Tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim = 41, d_model = 128, nhead = 4, num_layers = 2,dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.attention = nn.Linear(d_model, 1)\n",
    "\n",
    "        # self.d_model = d_model # Store d_model for classifier input dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2) # transformer expects (seq_len, batch, features)\n",
    "        x = self.transformer_encoder(x)\n",
    "        attn_scores = torch.softmax(self.attention(x), dim=0) \n",
    "        x = (x * attn_scores).sum(dim=0)\n",
    "      \n",
    "        # x = x[-1, :, :] # last time step\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivermarketos/miniconda3/envs/mybase/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated pos_weight: 6.2202 (Negatives=3446, Positives=554)\n",
      "Starting training for 20 epochs...\n",
      "Epoch 01/20\n",
      "  Train Loss: 1.4660\n",
      "  Val Loss: 1.7101, Val Acc (at 0.5): 0.1462, Val ROC-AUC: 0.7072\n",
      "  Validation ROC-AUC improved (0.0000 --> 0.7072). Saving model...\n",
      "Epoch 02/20\n",
      "  Train Loss: 1.2509\n",
      "  Val Loss: 1.5186, Val Acc (at 0.5): 0.3150, Val ROC-AUC: 0.7459\n",
      "  Validation ROC-AUC improved (0.7072 --> 0.7459). Saving model...\n",
      "Epoch 03/20\n",
      "  Train Loss: 1.1298\n",
      "  Val Loss: 1.4675, Val Acc (at 0.5): 0.4010, Val ROC-AUC: 0.7444\n",
      "  Validation ROC-AUC did not improve. Patience 1/5\n",
      "Epoch 04/20\n",
      "  Train Loss: 1.0343\n",
      "  Val Loss: 1.4427, Val Acc (at 0.5): 0.4395, Val ROC-AUC: 0.7522\n",
      "  Validation ROC-AUC improved (0.7459 --> 0.7522). Saving model...\n",
      "Epoch 05/20\n",
      "  Train Loss: 0.9333\n",
      "  Val Loss: 1.3989, Val Acc (at 0.5): 0.5075, Val ROC-AUC: 0.7745\n",
      "  Validation ROC-AUC improved (0.7522 --> 0.7745). Saving model...\n",
      "Epoch 06/20\n",
      "  Train Loss: 0.8774\n",
      "  Val Loss: 1.3896, Val Acc (at 0.5): 0.5457, Val ROC-AUC: 0.7797\n",
      "  Validation ROC-AUC improved (0.7745 --> 0.7797). Saving model...\n",
      "Epoch 07/20\n",
      "  Train Loss: 0.7747\n",
      "  Val Loss: 1.3242, Val Acc (at 0.5): 0.6018, Val ROC-AUC: 0.7801\n",
      "  Validation ROC-AUC improved (0.7797 --> 0.7801). Saving model...\n",
      "Epoch 08/20\n",
      "  Train Loss: 0.7584\n",
      "  Val Loss: 1.4991, Val Acc (at 0.5): 0.5633, Val ROC-AUC: 0.7716\n",
      "  Validation ROC-AUC did not improve. Patience 1/5\n",
      "Epoch 09/20\n",
      "  Train Loss: 0.7392\n",
      "  Val Loss: 1.4416, Val Acc (at 0.5): 0.6058, Val ROC-AUC: 0.7675\n",
      "  Validation ROC-AUC did not improve. Patience 2/5\n",
      "Epoch 10/20\n",
      "  Train Loss: 0.6967\n",
      "  Val Loss: 1.4729, Val Acc (at 0.5): 0.6068, Val ROC-AUC: 0.7833\n",
      "  Validation ROC-AUC improved (0.7801 --> 0.7833). Saving model...\n",
      "Epoch 11/20\n",
      "  Train Loss: 0.6512\n",
      "  Val Loss: 1.5527, Val Acc (at 0.5): 0.6095, Val ROC-AUC: 0.7700\n",
      "  Validation ROC-AUC did not improve. Patience 1/5\n",
      "Epoch 12/20\n",
      "  Train Loss: 0.6250\n",
      "  Val Loss: 1.7038, Val Acc (at 0.5): 0.6255, Val ROC-AUC: 0.7730\n",
      "  Validation ROC-AUC did not improve. Patience 2/5\n",
      "Epoch 13/20\n",
      "  Train Loss: 0.6437\n",
      "  Val Loss: 1.6091, Val Acc (at 0.5): 0.6005, Val ROC-AUC: 0.7717\n",
      "  Validation ROC-AUC did not improve. Patience 3/5\n",
      "Epoch 14/20\n",
      "  Train Loss: 0.5775\n",
      "  Val Loss: 1.6493, Val Acc (at 0.5): 0.6730, Val ROC-AUC: 0.7702\n",
      "  Validation ROC-AUC did not improve. Patience 4/5\n",
      "Epoch 15/20\n",
      "  Train Loss: 0.5127\n",
      "  Val Loss: 1.7931, Val Acc (at 0.5): 0.6975, Val ROC-AUC: 0.7783\n",
      "  Validation ROC-AUC did not improve. Patience 5/5\n",
      "Epoch 16/20\n",
      "  Train Loss: 0.5172\n",
      "  Val Loss: 1.8093, Val Acc (at 0.5): 0.7170, Val ROC-AUC: 0.7810\n",
      "  Validation ROC-AUC did not improve. Patience 6/5\n",
      "Epoch 17/20\n",
      "  Train Loss: 0.4630\n",
      "  Val Loss: 2.0032, Val Acc (at 0.5): 0.7352, Val ROC-AUC: 0.7769\n",
      "  Validation ROC-AUC did not improve. Patience 7/5\n",
      "Epoch 18/20\n",
      "  Train Loss: 0.3966\n",
      "  Val Loss: 2.0554, Val Acc (at 0.5): 0.7302, Val ROC-AUC: 0.7792\n",
      "  Validation ROC-AUC did not improve. Patience 8/5\n",
      "Epoch 19/20\n",
      "  Train Loss: 0.4373\n",
      "  Val Loss: 2.1955, Val Acc (at 0.5): 0.7518, Val ROC-AUC: 0.7778\n",
      "  Validation ROC-AUC did not improve. Patience 9/5\n",
      "Epoch 20/20\n",
      "  Train Loss: 0.3986\n",
      "  Val Loss: 2.2025, Val Acc (at 0.5): 0.7342, Val ROC-AUC: 0.7755\n",
      "  Validation ROC-AUC did not improve. Patience 10/5\n",
      "\n",
      "Training finished.\n",
      "Loading best model saved with ROC-AUC: 0.7833\n",
      "\n",
      "Evaluating on test set with the loaded best model...\n",
      "\n",
      "Test set evaluation:\n",
      "Test Loss: 1.7831, Test Acc (at 0.5): 0.4280, Test ROC-AUC: 0.7302\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesTransformer(num_layers=2, nhead=4)\n",
    "\n",
    "model_name = \"data/V5_transformer_model_2layers_4heads_128dim\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else  \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "n_samples = len(outcome_labels[0])\n",
    "n_positives = sum(outcome_labels[0])\n",
    "n_negatives = n_samples - n_positives\n",
    "\n",
    "pos_weight_val = n_negatives / n_positives\n",
    "\n",
    "train_dataset = MedicalTimeSeriesDataset(train_data, outcome_labels[0])\n",
    "val_dataset = MedicalTimeSeriesDataset(val_data, outcome_labels[1])\n",
    "test_dataset = MedicalTimeSeriesDataset(test_data, outcome_labels[2])\n",
    "\n",
    "batch_size = 32\n",
    "weights = np.where(outcome_labels[0] == 1, n_negatives / n_positives, 1.0)\n",
    "sampler = WeightedRandomSampler(weights.tolist(), num_samples=n_samples, replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler) # Use sampler for balanced sampling\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Calculated pos_weight: {pos_weight_val:.4f} (Negatives={n_negatives}, Positives={n_positives})\")\n",
    "pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # Added weight decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation/testing function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = [] # Store raw outputs (logits)\n",
    "    all_targets = [] # Store targets\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze() # raw logits\n",
    "\n",
    "            # Ensure y_batch has the same shape as outputs if squeeze() removed a dim\n",
    "            if outputs.dim() == 0: # Handle batch size of 1 if squeezed to scalar\n",
    "                 outputs = outputs.unsqueeze(0)\n",
    "            if y_batch.dim() > outputs.dim(): # Ensure y_batch has same shape as output\n",
    "                y_batch = y_batch.squeeze()\n",
    "            elif outputs.dim() > y_batch.dim(): # This shouldn't happen with .squeeze() above but safety check\n",
    "                outputs = outputs.squeeze() # Re-try squeeze if needed\n",
    "\n",
    "            # Ensure shapes match before loss calculation\n",
    "            if outputs.shape != y_batch.shape:\n",
    "                 # This indicates a potential issue elsewhere, maybe with batch size 1 handling\n",
    "                 print(f\"Shape mismatch: outputs {outputs.shape}, y_batch {y_batch.shape}\")\n",
    "                 # Decide how to handle: skip batch, reshape if possible, etc.\n",
    "                 # For now, we'll just report loss can't be computed for this batch\n",
    "                 continue # Skip this batch if shapes don't match\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "  # Apply sigmoid to logits to get probabilities for thresholding\n",
    "    probs = 1 / (1 + np.exp(-all_outputs)) # Sigmoid function\n",
    "    preds_labels = (probs >= 0.5).astype(int) # Threshold probabilities\n",
    "\n",
    "    # Ensure targets are integers for accuracy score\n",
    "    all_targets = all_targets.astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, preds_labels)\n",
    "    # ROC AUC can be calculated directly from logits (or probabilities)\n",
    "    # Check if there's more than one class present in targets for ROC AUC\n",
    "    if len(np.unique(all_targets)) > 1:\n",
    "        roc_auc = roc_auc_score(all_targets, all_outputs) # Use logits directly\n",
    "    else:\n",
    "        roc_auc = 0.5 # Or np.nan, indicating AUC is not defined\n",
    "        print(f\"Warning: Only one class present in targets. ROC AUC set to {roc_auc}\")\n",
    "\n",
    "\n",
    "    return avg_loss, accuracy, roc_auc\n",
    "\n",
    "    \n",
    "num_epochs = 20 # Increase epochs, rely on saving the best\n",
    "best_val_roc_auc = 0.0 # Initialize low for maximization\n",
    "patience_counter = 0\n",
    "patience = 5 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, val_roc_auc = evaluate(model, val_loader, criterion, device)\n",
    "    scheduler.step(val_roc_auc) # Adjust learning rate based on validation ROC AUC\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch:02d}/{num_epochs}')\n",
    "    print(f'  Train Loss: {train_loss:.4f}')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc (at 0.5): {val_accuracy:.4f}, Val ROC-AUC: {val_roc_auc:.4f}') # Label accuracy as potentially misleading\n",
    "\n",
    "    # Save best model based on validation ROC-AUC\n",
    "    if val_roc_auc > best_val_roc_auc:\n",
    "        print(f'  Validation ROC-AUC improved ({best_val_roc_auc:.4f} --> {val_roc_auc:.4f}). Saving model...')\n",
    "        best_val_roc_auc = val_roc_auc # Correctly update the best AUC score\n",
    "        torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "        patience_counter = 0 # Reset patience counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  Validation ROC-AUC did not improve. Patience {patience_counter}/{patience}')\n",
    "\n",
    "   \n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# Load best model (make sure the file exists)\n",
    "print(f\"Loading best model saved with ROC-AUC: {best_val_roc_auc:.4f}\")\n",
    "try:\n",
    "    model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{model_name}.pt' not found. Was a model ever saved?\")\n",
    "    # Handle error appropriately - maybe exit or proceed with the last state of the model?\n",
    "\n",
    "# Evaluate on test set\n",
    "print('\\nEvaluating on test set with the loaded best model...')\n",
    "test_loss, test_accuracy, test_roc_auc = evaluate(model, test_loader, criterion, device)\n",
    "print('\\nTest set evaluation:')\n",
    "# Remind that accuracy uses 0.5 threshold\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc (at 0.5): {test_accuracy:.4f}, Test ROC-AUC: {test_roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding optimal threshold on validation set...\n",
      "Optimal threshold based on max F1 score on validation set: 0.9496\n",
      "Optimal threshold based on max Youden's J on validation set: 0.9234\n",
      "\n",
      "Evaluating on test set using threshold: 0.9496\n",
      "\n",
      "Test set evaluation (with optimized threshold):\n",
      "- Test Loss: 1.4269\n",
      "- Test ROC-AUC: 0.7483\n",
      "- Test Accuracy: 0.7943\n",
      "- Test Precision: 0.3062\n",
      "- Test Recall: 0.3214\n",
      "- Test F1-Score: 0.3136\n"
     ]
    }
   ],
   "source": [
    "# --- After loading the best model ---\n",
    "print(\"\\nFinding optimal threshold on validation set...\")\n",
    "model.eval() # Ensure model is in eval mode\n",
    "val_logits = []\n",
    "val_targets_list = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        val_logits.extend(outputs.cpu().numpy())\n",
    "        val_targets_list.extend(y_batch.cpu().numpy())\n",
    "\n",
    "val_logits = np.array(val_logits)\n",
    "val_targets_np = np.array(val_targets_list).astype(int)\n",
    "val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "# Method 1: Maximize F1 score\n",
    "precision, recall, thresholds_pr = precision_recall_curve(val_targets_np, val_probs)\n",
    "# Calculate F1 score, handling potential division by zero\n",
    "f1_scores = 2 * recall * precision / (recall + precision + 1e-8)\n",
    "# thresholds_pr correspond to precision/recall pairs, need to adjust index\n",
    "optimal_idx_f1 = np.argmax(f1_scores)\n",
    "optimal_threshold_f1 = thresholds_pr[optimal_idx_f1]\n",
    "print(f\"Optimal threshold based on max F1 score on validation set: {optimal_threshold_f1:.4f}\")\n",
    "\n",
    "# Method 2: Maximize Youden's J (sensitivity + specificity - 1)\n",
    "fpr, tpr, thresholds_roc = roc_curve(val_targets_np, val_probs)\n",
    "youden_j = tpr - fpr\n",
    "optimal_idx_j = np.argmax(youden_j)\n",
    "optimal_threshold_j = thresholds_roc[optimal_idx_j]\n",
    "print(f\"Optimal threshold based on max Youden's J on validation set: {optimal_threshold_j:.4f}\")\n",
    "\n",
    "# Choose one threshold (e.g., from F1)\n",
    "# optimal_threshold =  optimal_threshold_j \n",
    "# or\n",
    "optimal_threshold =  optimal_threshold_f1 \n",
    "\n",
    "def evaluate_with_threshold(model, loader, criterion, device, threshold):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = [] # Store raw outputs (logits)\n",
    "    all_targets = [] # Store targets\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # (Same loop as before to get all_outputs and all_targets)\n",
    "        for X_batch, y_batch in loader:\n",
    "            # ... (identical data loading and model prediction) ...\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze() # raw logits\n",
    "\n",
    "            if outputs.dim() == 0: outputs = outputs.unsqueeze(0)\n",
    "            if y_batch.dim() > outputs.dim(): y_batch = y_batch.squeeze()\n",
    "            elif outputs.dim() > y_batch.dim(): outputs = outputs.squeeze()\n",
    "\n",
    "            if outputs.shape != y_batch.shape:\n",
    "                 print(f\"Shape mismatch: outputs {outputs.shape}, y_batch {y_batch.shape}\")\n",
    "                 continue\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    all_targets = np.array(all_targets)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "    # Apply sigmoid and OPTIMAL threshold\n",
    "    probs = 1 / (1 + np.exp(-all_outputs))\n",
    "    preds_labels = (probs >= threshold).astype(int) # Use the optimal threshold\n",
    "\n",
    "    all_targets = all_targets.astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, preds_labels)\n",
    "    roc_auc = roc_auc_score(all_targets, all_outputs) if len(np.unique(all_targets)) > 1 else 0.5\n",
    "\n",
    "    # Calculate other metrics\n",
    "    precision = precision_score(all_targets, preds_labels, zero_division=0)\n",
    "    recall = recall_score(all_targets, preds_labels, zero_division=0)\n",
    "    f1 = f1_score(all_targets, preds_labels, zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, roc_auc, precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluate on test set using the found threshold\n",
    "print(f'\\nEvaluating on test set using threshold: {optimal_threshold:.4f}')\n",
    "test_loss, test_accuracy, test_roc_auc, test_precision, test_recall, test_f1 = evaluate_with_threshold(\n",
    "    model, test_loader, criterion, device, optimal_threshold\n",
    ")\n",
    "\n",
    "print('\\nTest set evaluation (with optimized threshold):')\n",
    "print(f'- Test Loss: {test_loss:.4f}')\n",
    "print(f'- Test ROC-AUC: {test_roc_auc:.4f}')\n",
    "print(f'- Test Accuracy: {test_accuracy:.4f}')\n",
    "print(f'- Test Precision: {test_precision:.4f}')\n",
    "print(f'- Test Recall: {test_recall:.4f}')\n",
    "print(f'- Test F1-Score: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finding optimal threshold on validation set...\n",
    "\n",
    "Using standard scaling:\n",
    "- Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.9818\n",
    "- Test ROC-AUC: 0.7919\n",
    "- Test Accuracy: 0.6032\n",
    "- Test Precision: 0.2420\n",
    "- Test Recall: 0.8034\n",
    "- Test F1-Score: 0.3720\n",
    "\n",
    "Robust scaling:\n",
    "Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.8851\n",
    "- Test ROC-AUC: 0.7476\n",
    "- Test Accuracy: 0.5727\n",
    "- Test Precision: 0.2282\n",
    "- Test Recall: 0.8068\n",
    "- Test F1-Score: 0.3558\n",
    "\n",
    "Scikit robust scaling\n",
    "Evaluating on test set using threshold: 0.4240\n",
    "\n",
    "Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.8678\n",
    "- Test ROC-AUC: 0.7493\n",
    "- Test Accuracy: 0.7432\n",
    "- Test Precision: 0.3065\n",
    "- Test Recall: 0.5983\n",
    "- Test F1-Score: 0.4053\n",
    "\n",
    "Scikit standard scaling\n",
    "Optimal threshold based on max F1 score on validation set: 0.5811\n",
    "Optimal threshold based on max Youden's J on validation set: 0.4707\n",
    "\n",
    "Evaluating on test set using threshold: 0.5811\n",
    "\n",
    "Test set evaluation (with optimized threshold):\n",
    "- Test Loss: 0.8664\n",
    "- Test ROC-AUC: 0.7889\n",
    "- Test Accuracy: 0.6062\n",
    "- Test Precision: 0.2446\n",
    "- Test Recall: 0.8103\n",
    "- Test F1-Score: 0.3757"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
