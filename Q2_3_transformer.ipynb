{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 4\n"
     ]
    }
   ],
   "source": [
    "static_variables = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "static_variables_we_want = ['Age', 'Gender', 'Height', 'Weight']\n",
    "all_variables = ['Weight', 'Age', 'TroponinI', 'DiasABP', 'MechVent', 'HCO3', 'Cholesterol', 'HCT', 'SaO2', 'WBC', 'SysABP', 'Urine', 'ICUType', 'Gender', 'ALP', 'Creatinine', 'K', 'AST', 'Glucose', 'RespRate', 'MAP', 'FiO2', 'BUN', 'Na', 'Bilirubin', 'TroponinT', 'PaCO2', 'Height', 'GCS', 'HR', 'pH', 'PaO2', 'Lactate', 'ALT', 'NISysABP', 'RecordID', 'Platelets', 'Temp', 'Mg', 'NIDiasABP', 'Albumin', 'NIMAP']\n",
    "dyn_variables = [x for x in all_variables if x not in static_variables]\n",
    "dyn_variables.append('Weight_VAR')\n",
    "\n",
    "print(len(dyn_variables), len(static_variables_we_want))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"data/set-a_no_nan.parquet\")\n",
    "val_df = pd.read_parquet(\"data/set-b_no_nan.parquet\")\n",
    "test_df = pd.read_parquet(\"data/set-c_no_nan.parquet\")\n",
    "\n",
    "outcomes_a_df = pd.read_csv(\"data/Outcomes-a.txt\", sep=\",\")\n",
    "outcomes_b_df = pd.read_csv(\"data/Outcomes-b.txt\", sep=\",\")\n",
    "outcomes_c_df = pd.read_csv(\"data/Outcomes-c.txt\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,) <class 'numpy.ndarray'>\n",
      "(4000,) <class 'numpy.ndarray'>\n",
      "(4000,) <class 'numpy.ndarray'>\n",
      "All records in outcomes and data are in same order\n"
     ]
    }
   ],
   "source": [
    "tmp_outcomes = [outcomes_a_df, outcomes_b_df, outcomes_c_df]\n",
    "\n",
    "data_df = [train_df, val_df, test_df]\n",
    "\n",
    "outcome_labels = []\n",
    "for i in range(3):\n",
    "    outcome_records = tmp_outcomes[i]['RecordID'].unique().astype(int)\n",
    "    train_records = data_df[i]['RecordID'].unique().astype(int)\n",
    "    assert (outcome_records - train_records == 0).all(), \"Mismatch: expected difference of 0 between outcome_records and train_records.\"\n",
    "\n",
    "    outcome_labels.append(tmp_outcomes[i][\"In-hospital_death\"].values) # 1 if patient died in hospital, 0 otherwise\n",
    "    print(outcome_labels[i].shape, type(outcome_labels[i]))\n",
    "print(\"All records in outcomes and data are in same order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 49, 41) (4000, 49, 41) (4000, 49, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert dfs into numpy arrays\n",
    "def convert_df_to_np(df):\n",
    "    dfs = []\n",
    "    for record_id in df['RecordID'].unique():\n",
    "        df_tmp = df[df['RecordID'] == record_id]\n",
    "        df_tmp = df_tmp.drop(columns=['RecordID', \"Time\"])\n",
    "        arr = df_tmp.to_numpy()\n",
    "        dfs.append(arr)\n",
    "\n",
    "    # convert list of dfs to list of tensors\n",
    "    train_data = np.array(dfs)\n",
    "    return train_data\n",
    "\n",
    "train_data = convert_df_to_np(data_df[0])\n",
    "val_data = convert_df_to_np(data_df[1])\n",
    "test_data = convert_df_to_np(data_df[2])\n",
    "\n",
    "# Standardize data\n",
    "mean = train_data.mean(axis=(0,1), keepdims=True)\n",
    "std = train_data.std(axis=(0,1), keepdims=True)\n",
    "std[std == 0] = 1.0\n",
    "\n",
    "\n",
    "# 2. Standardize data\n",
    "train_data = (train_data - mean) / std\n",
    "val_data = (val_data - mean) / std  \n",
    "test_data = (test_data - mean) / std  #\n",
    "\n",
    "print(train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MedicalTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MedicalTimeSeriesDataset(train_data, outcome_labels[0])\n",
    "val_dataset = MedicalTimeSeriesDataset(val_data, outcome_labels[1])\n",
    "test_dataset = MedicalTimeSeriesDataset(test_data, outcome_labels[2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_length=49):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, d_model) # max_length: number tokens, d_model: dimension of each token (embedding dim)\n",
    "\n",
    "        position = torch.arange(0, max_length).unsqueeze(1) # shape (max_length, 1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2)* -(torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        self.pe: torch.Tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim = 41, d_model = 128, nhead = 4, num_layers = 2,dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.d_model = d_model # Store d_model for classifier input dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2) # transformer expects (seq_len, batch, features)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # --- Global Average Pooling ---\n",
    "        # Average across the sequence length dimension (dim=0)\n",
    "        x = x.mean(dim=0) # (batch, d_model) e.g., (64, 128)\n",
    "        # --- Alternative: Use Last Time Step ---\n",
    "        # x = x[-1, :, :] # (batch, d_model) e.g., (64, 128)\n",
    "        # ---------------------------------------\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated pos_weight: 6.2202 (Negatives=3446, Positives=554)\n",
      "Epoch 01/7\n",
      "  Train Loss: 1.0197\n",
      "  Val Loss: 0.9501, Val Acc: 0.6082, Val ROC-AUC: 0.8246\n",
      "Epoch 02/7\n",
      "  Train Loss: 0.9017\n",
      "  Val Loss: 0.8739, Val Acc: 0.7127, Val ROC-AUC: 0.8366\n",
      "Epoch 03/7\n",
      "  Train Loss: 0.8418\n",
      "  Val Loss: 0.8554, Val Acc: 0.6897, Val ROC-AUC: 0.8505\n",
      "Epoch 04/7\n",
      "  Train Loss: 0.7998\n",
      "  Val Loss: 0.8760, Val Acc: 0.6943, Val ROC-AUC: 0.8485\n",
      "Epoch 05/7\n",
      "  Train Loss: 0.7788\n",
      "  Val Loss: 0.8550, Val Acc: 0.7893, Val ROC-AUC: 0.8527\n",
      "Epoch 06/7\n",
      "  Train Loss: 0.7560\n",
      "  Val Loss: 0.8486, Val Acc: 0.7428, Val ROC-AUC: 0.8485\n",
      "Epoch 07/7\n",
      "  Train Loss: 0.7101\n",
      "  Val Loss: 0.8803, Val Acc: 0.8003, Val ROC-AUC: 0.8526\n",
      "\n",
      "Test set evaluation:\n",
      "Test Loss: 1.1669, Test Acc: 0.4860, Test ROC-AUC: 0.7798\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesTransformer(num_layers=3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else  \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "n_samples = len(outcome_labels[0])\n",
    "n_positives = sum(outcome_labels[0])\n",
    "n_negatives = n_samples - n_positives\n",
    "\n",
    "if n_positives > 0:\n",
    "    pos_weight_val = n_negatives / n_positives\n",
    "else:\n",
    "    pos_weight_val = 1.0 # Or handle as an error if appropriate\n",
    "\n",
    "print(f\"Calculated pos_weight: {pos_weight_val:.4f} (Negatives={n_negatives}, Positives={n_positives})\")\n",
    "pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # Added weight decay\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation/testing function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = [] # Store raw outputs (logits)\n",
    "    all_targets = [] # Store targets\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze() # raw logits\n",
    "\n",
    "            # Ensure y_batch has the same shape as outputs if squeeze() removed a dim\n",
    "            if outputs.dim() == 0: # Handle batch size of 1 if squeezed to scalar\n",
    "                 outputs = outputs.unsqueeze(0)\n",
    "            if y_batch.dim() > outputs.dim(): # Ensure y_batch has same shape as output\n",
    "                y_batch = y_batch.squeeze()\n",
    "            elif outputs.dim() > y_batch.dim(): # This shouldn't happen with .squeeze() above but safety check\n",
    "                outputs = outputs.squeeze() # Re-try squeeze if needed\n",
    "\n",
    "            # Ensure shapes match before loss calculation\n",
    "            if outputs.shape != y_batch.shape:\n",
    "                 # This indicates a potential issue elsewhere, maybe with batch size 1 handling\n",
    "                 print(f\"Shape mismatch: outputs {outputs.shape}, y_batch {y_batch.shape}\")\n",
    "                 # Decide how to handle: skip batch, reshape if possible, etc.\n",
    "                 # For now, we'll just report loss can't be computed for this batch\n",
    "                 continue # Skip this batch if shapes don't match\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "  # Apply sigmoid to logits to get probabilities for thresholding\n",
    "    probs = 1 / (1 + np.exp(-all_outputs)) # Sigmoid function\n",
    "    preds_labels = (probs >= 0.5).astype(int) # Threshold probabilities\n",
    "\n",
    "    # Ensure targets are integers for accuracy score\n",
    "    all_targets = all_targets.astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, preds_labels)\n",
    "    # ROC AUC can be calculated directly from logits (or probabilities)\n",
    "    # Check if there's more than one class present in targets for ROC AUC\n",
    "    if len(np.unique(all_targets)) > 1:\n",
    "        roc_auc = roc_auc_score(all_targets, all_outputs) # Use logits directly\n",
    "    else:\n",
    "        roc_auc = 0.5 # Or np.nan, indicating AUC is not defined\n",
    "        print(f\"Warning: Only one class present in targets. ROC AUC set to {roc_auc}\")\n",
    "\n",
    "\n",
    "    return avg_loss, accuracy, roc_auc\n",
    "\n",
    "    \n",
    "\n",
    "# Training loop\n",
    "num_epochs = 7\n",
    "best_val_roc_auc = float('inf')\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, val_roc_auc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f'Epoch {epoch:02d}/{num_epochs}')\n",
    "    print(f'  Train Loss: {train_loss:.4f}')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val ROC-AUC: {val_roc_auc:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if val_roc_auc < best_val_roc_auc:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_transformer_model.pt')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_transformer_model.pt'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy, test_roc_auc = evaluate(model, test_loader, criterion, device)\n",
    "print('\\nTest set evaluation:')\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}, Test ROC-AUC: {test_roc_auc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
