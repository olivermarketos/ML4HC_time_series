{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.style.use('default')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set globally to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Add this near the top of your script ---\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility in PyTorch, NumPy, and Python.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value) # Numpy module\n",
    "    torch.manual_seed(seed_value) # PyTorch CPU seeding\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "        # Configure CuDNN for deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # Optional: Newer PyTorch versions might require this for full determinism\n",
    "        # Note: This can sometimes throw errors if a deterministic implementation isn't available\n",
    "        # try:\n",
    "        #     torch.use_deterministic_algorithms(True)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Warning: Could not enable deterministic algorithms: {e}\")\n",
    "        # Optional: Sometimes needed for deterministic matrix multiplication\n",
    "        # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    print(f\"Seed set globally to {seed_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Call this function very early in your script ---\n",
    "SEED = 42 # Choose your desired seed value\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 4\n"
     ]
    }
   ],
   "source": [
    "static_variables = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "categorical_vars = [\"Gender\",\"GCS\", \"MechVent\"]\n",
    "static_variables_we_want = ['Age', 'Gender', 'Height', 'Weight']\n",
    "all_variables = ['Weight', 'Age', 'TroponinI', 'DiasABP', 'MechVent', 'HCO3', 'Cholesterol', 'HCT', 'SaO2', 'WBC', 'SysABP', 'Urine', 'ICUType', 'Gender', 'ALP', 'Creatinine', 'K', 'AST', 'Glucose', 'RespRate', 'MAP', 'FiO2', 'BUN', 'Na', 'Bilirubin', 'TroponinT', 'PaCO2', 'Height', 'GCS', 'HR', 'pH', 'PaO2', 'Lactate', 'ALT', 'NISysABP', 'RecordID', 'Platelets', 'Temp', 'Mg', 'NIDiasABP', 'Albumin', 'NIMAP']\n",
    "dyn_variables = [x for x in all_variables if x not in static_variables]\n",
    "dyn_variables.append('Weight_VAR')\n",
    "\n",
    "print(len(dyn_variables), len(static_variables_we_want))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from data/set-a_no_nan.parquet, shape: (196000, 43)\n",
      "Loaded data from data/set-b_no_nan.parquet, shape: (196000, 43)\n",
      "Loaded data from data/set-c_no_nan.parquet, shape: (196000, 43)\n",
      "Loaded 4000 outcomes from data/Outcomes-a.txt\n",
      "Loaded 4000 outcomes from data/Outcomes-b.txt\n",
      "Loaded 4000 outcomes from data/Outcomes-c.txt\n"
     ]
    }
   ],
   "source": [
    "from data_loaders import load_outcomes, load_processed_data\n",
    "\n",
    "train_df = load_processed_data(\"data/set-a_no_nan.parquet\")\n",
    "val_df = load_processed_data(\"data/set-b_no_nan.parquet\")\n",
    "test_df = load_processed_data(\"data/set-c_no_nan.parquet\")\n",
    "\n",
    "outcomes_a_dict = load_outcomes(\"data/Outcomes-a.txt\")\n",
    "outcomes_b_dict = load_outcomes(\"data/Outcomes-b.txt\")\n",
    "outcomes_c_dict = load_outcomes(\"data/Outcomes-c.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RecordID', 'Time', 'Age', 'Gender', 'Height', 'Weight', 'Albumin',\n",
      "       'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN', 'Cholesterol', 'Creatinine',\n",
      "       'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3', 'HCT', 'HR', 'K',\n",
      "       'Lactate', 'Mg', 'MAP', 'MechVent', 'Na', 'NIDiasABP', 'NIMAP',\n",
      "       'NISysABP', 'PaCO2', 'PaO2', 'pH', 'Platelets', 'RespRate', 'SaO2',\n",
      "       'SysABP', 'Temp', 'TroponinI', 'TroponinT', 'Urine', 'WBC',\n",
      "       'Weight_VAR'],\n",
      "      dtype='object', name='Parameter')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Aligning Labels with Data Order ---\n",
      "\n",
      "Processing Train Set:\n",
      "  Order of unique RecordIDs based on data: [132539 132540 132541 132543 132545 132547 132548 132551 132554 132555]... (Total: 4000)\n",
      "  Set of RecordIDs matches.\n",
      "  Aligned labels created. Shape: (4000,), Type: <class 'numpy.ndarray'>\n",
      "\n",
      "Processing Validation Set:\n",
      "  Order of unique RecordIDs based on data: [142675 142676 142680 142683 142688 142690 142691 142692 142693 142694]... (Total: 4000)\n",
      "  Set of RecordIDs matches.\n",
      "  Aligned labels created. Shape: (4000,), Type: <class 'numpy.ndarray'>\n",
      "\n",
      "Processing Test Set:\n",
      "  Order of unique RecordIDs based on data: [152871 152873 152875 152878 152882 152884 152885 152886 152887 152890]... (Total: 4000)\n",
      "  Set of RecordIDs matches.\n",
      "  Aligned labels created. Shape: (4000,), Type: <class 'numpy.ndarray'>\n",
      "\n",
      "--- Final Check ---\n",
      "All checks passed. Labels are aligned with the unique RecordID order from data DataFrames.\n"
     ]
    }
   ],
   "source": [
    "outcome_dicts = [outcomes_a_dict, outcomes_b_dict, outcomes_c_dict] # USE outcomes_b_dict for index 1\n",
    "data_dfs = [train_df, val_df, test_df]\n",
    "set_names = [\"Train\", \"Validation\", \"Test\"] # For printing\n",
    "\n",
    "outcome_labels_aligned = [] # Use a new name to avoid confusion\n",
    "\n",
    "print(\"--- Aligning Labels with Data Order ---\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nProcessing {set_names[i]} Set:\")\n",
    "    current_outcome_dict = outcome_dicts[i]\n",
    "    current_data_df = data_dfs[i]\n",
    "\n",
    "    # 1. Get the DEFINITIVE order of RecordIDs from the data DataFrame\n",
    "    # This order dictates how your features will be structured for the dataset\n",
    "    data_record_ids_ordered = current_data_df['RecordID'].unique().astype(int)\n",
    "    print(f\"  Order of unique RecordIDs based on data: {data_record_ids_ordered[:10]}... (Total: {len(data_record_ids_ordered)})\")\n",
    "\n",
    "    # 2. Check if all necessary outcome IDs are present (Set Check - Good Sanity Check)\n",
    "    outcome_record_ids_set = set(current_outcome_dict.keys())\n",
    "    data_record_ids_set = set(data_record_ids_ordered)\n",
    "\n",
    "    if outcome_record_ids_set != data_record_ids_set:\n",
    "        print(f\"  ERROR: Set of RecordIDs mismatch for {set_names[i]} set.\")\n",
    "        missing_in_outcomes = data_record_ids_set - outcome_record_ids_set\n",
    "        missing_in_data = outcome_record_ids_set - data_record_ids_set\n",
    "        if missing_in_outcomes:\n",
    "            print(f\"  IDs in data but not outcomes: {missing_in_outcomes}\")\n",
    "        if missing_in_data:\n",
    "            print(f\"  IDs in outcomes but not data: {missing_in_data}\")\n",
    "        raise ValueError(f\"RecordID sets do not match for {set_names[i]} set. Cannot align labels reliably.\")\n",
    "    else:\n",
    "        print(f\"  Set of RecordIDs matches.\")\n",
    "\n",
    "    # 3. Create the labels array by looking up IDs IN THE ORDER defined by the data\n",
    "    try:\n",
    "        current_labels = np.array(\n",
    "            [current_outcome_dict[record_id] for record_id in data_record_ids_ordered],\n",
    "            dtype=np.float32 # Use float for BCELoss compatibility\n",
    "        )\n",
    "        outcome_labels_aligned.append(current_labels)\n",
    "        print(f\"  Aligned labels created. Shape: {current_labels.shape}, Type: {type(current_labels)}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"  ERROR: RecordID {e} found in data but missing from outcome dictionary during alignment!\")\n",
    "        raise ValueError(f\"KeyError during label alignment for {set_names[i]} set.\")\n",
    "    except Exception as e:\n",
    "         print(f\"  ERROR: An unexpected error occurred during label alignment for {set_names[i]}: {e}\")\n",
    "         raise e\n",
    "\n",
    "\n",
    "# Final verification (optional)\n",
    "print(\"\\n--- Final Check ---\")\n",
    "if len(outcome_labels_aligned) == 3:\n",
    "    assert len(data_dfs[0]['RecordID'].unique()) == len(outcome_labels_aligned[0]), f\"Train length mismatch: Data {len(data_dfs[0]['RecordID'].unique())}, Labels {len(outcome_labels_aligned[0])}\"\n",
    "    assert len(data_dfs[1]['RecordID'].unique()) == len(outcome_labels_aligned[1]), f\"Validation length mismatch: Data {len(data_dfs[1]['RecordID'].unique())}, Labels {len(outcome_labels_aligned[1])}\"\n",
    "    assert len(data_dfs[2]['RecordID'].unique()) == len(outcome_labels_aligned[2]), f\"Test length mismatch: Data {len(data_dfs[2]['RecordID'].unique())}, Labels {len(outcome_labels_aligned[2])}\"\n",
    "    print(\"All checks passed. Labels are aligned with the unique RecordID order from data DataFrames.\")\n",
    "else:\n",
    "    print(\"Could not perform final length check due to earlier errors.\")\n",
    "\n",
    "# Now 'outcome_labels_aligned' contains [y_train, y_val, y_test] NumPy arrays\n",
    "# correctly ordered according to the unique patient order in train_df, val_df, test_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Standard scaled train data shape: (4000, 49, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert dfs into numpy arrays\n",
    "def convert_df_to_np(df):\n",
    "    dfs = []\n",
    "    for record_id in df['RecordID'].unique():\n",
    "        df_tmp = df[df['RecordID'] == record_id]\n",
    "        df_tmp = df_tmp.drop(columns=['RecordID', \"Time\"])\n",
    "        arr = df_tmp.to_numpy()\n",
    "        dfs.append(arr)\n",
    "\n",
    "    # convert list of dfs to list of tensors\n",
    "    train_data = np.array(dfs)\n",
    "    return train_data\n",
    "\n",
    "\n",
    "train_data = convert_df_to_np(data_df[0])\n",
    "val_data = convert_df_to_np(data_df[1])\n",
    "test_data = convert_df_to_np(data_df[2])\n",
    "\n",
    "y_train = outcome_labels[0]\n",
    "y_val = outcome_labels[1]\n",
    "y_test = outcome_labels[2]\n",
    "\n",
    "\n",
    "# Standardize data\n",
    "# Original shape: (n_patients, n_timepoints, n_features)\n",
    "n_patients, n_timepoints, n_features = train_data.shape\n",
    "\n",
    "# Reshape to 2D: (n_patients * n_timepoints, n_features)\n",
    "train_data_2d = train_data.reshape(-1, n_features)\n",
    "val_data_2d = val_data.reshape(-1, n_features)\n",
    "test_data_2d = test_data.reshape(-1, n_features)\n",
    "\n",
    "# Initialize and fit the scaler ONLY on training data\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_data_2d)\n",
    "\n",
    "# Transform all datasets\n",
    "train_scaled_2d = scaler.transform(train_data_2d)\n",
    "val_scaled_2d = scaler.transform(val_data_2d)\n",
    "test_scaled_2d = scaler.transform(test_data_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "train_data = train_scaled_2d.reshape(n_patients, n_timepoints, n_features)\n",
    "val_data = val_scaled_2d.reshape(val_data.shape)\n",
    "test_data = test_scaled_2d.reshape(test_data.shape)\n",
    "\n",
    "print(\"Sklearn Standard scaled train data shape:\", train_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Evaluation Functions ---\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (batch_X, batch_y) in enumerate(loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        # Gradient clipping (optional but can help stability)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print progress (optional)\n",
    "        # if (i + 1) % 50 == 0:\n",
    "        #      elapsed = time.time() - start_time\n",
    "        #      print(f'  Batch {i+1}/{len(loader)}, Loss: {loss.item():.4f}, Time: {elapsed:.2f}s')\n",
    "        #      start_time = time.time() # Reset timer\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device, return_probs=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_X) # raw logits\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_logits.extend(outputs.cpu())\n",
    "            all_labels.extend(batch_y.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_logits = torch.cat(all_logits).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy().flatten()\n",
    "\n",
    "    all_probs = 1/ (1+ np.exp(-all_logits)).flatten() # Sigmoid to get probabilities\n",
    "\n",
    "    all_preds_05 = np.where(all_probs > 0.5, 1, 0).astype(int) # Convert probabilities to binary predictions\n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds_05) # Use probabilities for AUC if needed: roc_auc_score(all_labels, probs.flatten())\n",
    "    except ValueError:\n",
    "        print(\"Warning: ROC AUC calculation failed. Likely only one class present in this evaluation batch/set.\")\n",
    "        auc = 0.0 # Or handle as appropriate\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds_05)\n",
    "    acc = accuracy_score(all_labels, all_preds_05)\n",
    "    prec = precision_score(all_labels, all_preds_05, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds_05, zero_division=0)\n",
    "    conf_mat = confusion_matrix(all_labels, all_preds_05)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'conf_matrix': conf_mat\n",
    "    }\n",
    "    if return_probs:\n",
    "        return metrics, all_probs, all_labels\n",
    "    else:\n",
    "        return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivermarketos/miniconda3/envs/mybase/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Time: 3.13s\n",
      "  Train Loss: 0.6457\n",
      "  Val Loss: 0.6988 | Val AUC: 0.6592 | Val F1: 0.3411\n",
      "  Val Accuracy: 0.5703 | Val Precision: 0.2180 | Val Recall: 0.7835\n",
      "  Val Confusion Matrix:\n",
      "[[1836 1596]\n",
      " [ 123  445]]\n",
      "  * Best model saved based on Val AUC: 0.6592 at epoch 1\n",
      "Epoch 2/30 | Time: 2.73s\n",
      "  Train Loss: 0.5709\n",
      "  Val Loss: 0.5760 | Val AUC: 0.7044 | Val F1: 0.3953\n",
      "  Val Accuracy: 0.6780 | Val Precision: 0.2695 | Val Recall: 0.7412\n",
      "  Val Confusion Matrix:\n",
      "[[2291 1141]\n",
      " [ 147  421]]\n",
      "  * Best model saved based on Val AUC: 0.7044 at epoch 2\n",
      "Epoch 3/30 | Time: 2.71s\n",
      "  Train Loss: 0.5584\n",
      "  Val Loss: 0.4943 | Val AUC: 0.7149 | Val F1: 0.4331\n",
      "  Val Accuracy: 0.7552 | Val Precision: 0.3227 | Val Recall: 0.6585\n",
      "  Val Confusion Matrix:\n",
      "[[2647  785]\n",
      " [ 194  374]]\n",
      "  * Best model saved based on Val AUC: 0.7149 at epoch 3\n",
      "Epoch 4/30 | Time: 2.77s\n",
      "  Train Loss: 0.5189\n",
      "  Val Loss: 0.4475 | Val AUC: 0.7131 | Val F1: 0.4437\n",
      "  Val Accuracy: 0.7788 | Val Precision: 0.3451 | Val Recall: 0.6215\n",
      "  Val Confusion Matrix:\n",
      "[[2762  670]\n",
      " [ 215  353]]\n",
      "Epoch 5/30 | Time: 2.71s\n",
      "  Train Loss: 0.4939\n",
      "  Val Loss: 0.5048 | Val AUC: 0.7146 | Val F1: 0.4280\n",
      "  Val Accuracy: 0.7448 | Val Precision: 0.3139 | Val Recall: 0.6725\n",
      "  Val Confusion Matrix:\n",
      "[[2597  835]\n",
      " [ 186  382]]\n",
      "Epoch 6/30 | Time: 2.72s\n",
      "  Train Loss: 0.4831\n",
      "  Val Loss: 0.5232 | Val AUC: 0.7122 | Val F1: 0.4229\n",
      "  Val Accuracy: 0.7380 | Val Precision: 0.3077 | Val Recall: 0.6761\n",
      "  Val Confusion Matrix:\n",
      "[[2568  864]\n",
      " [ 184  384]]\n",
      "Epoch 7/30 | Time: 2.70s\n",
      "  Train Loss: 0.4886\n",
      "  Val Loss: 0.5271 | Val AUC: 0.7252 | Val F1: 0.4272\n",
      "  Val Accuracy: 0.7225 | Val Precision: 0.3022 | Val Recall: 0.7289\n",
      "  Val Confusion Matrix:\n",
      "[[2476  956]\n",
      " [ 154  414]]\n",
      "  * Best model saved based on Val AUC: 0.7252 at epoch 7\n",
      "Epoch 8/30 | Time: 2.77s\n",
      "  Train Loss: 0.4378\n",
      "  Val Loss: 0.5781 | Val AUC: 0.7255 | Val F1: 0.4223\n",
      "  Val Accuracy: 0.7093 | Val Precision: 0.2941 | Val Recall: 0.7482\n",
      "  Val Confusion Matrix:\n",
      "[[2412 1020]\n",
      " [ 143  425]]\n",
      "  * Best model saved based on Val AUC: 0.7255 at epoch 8\n",
      "Epoch 9/30 | Time: 2.72s\n",
      "  Train Loss: 0.4268\n",
      "  Val Loss: 0.4738 | Val AUC: 0.7138 | Val F1: 0.4381\n",
      "  Val Accuracy: 0.7672 | Val Precision: 0.3333 | Val Recall: 0.6391\n",
      "  Val Confusion Matrix:\n",
      "[[2706  726]\n",
      " [ 205  363]]\n",
      "Epoch 10/30 | Time: 2.71s\n",
      "  Train Loss: 0.4152\n",
      "  Val Loss: 0.4922 | Val AUC: 0.7148 | Val F1: 0.4349\n",
      "  Val Accuracy: 0.7590 | Val Precision: 0.3260 | Val Recall: 0.6532\n",
      "  Val Confusion Matrix:\n",
      "[[2665  767]\n",
      " [ 197  371]]\n",
      "Epoch 11/30 | Time: 2.71s\n",
      "  Train Loss: 0.4292\n",
      "  Val Loss: 0.5089 | Val AUC: 0.7140 | Val F1: 0.4276\n",
      "  Val Accuracy: 0.7450 | Val Precision: 0.3138 | Val Recall: 0.6708\n",
      "  Val Confusion Matrix:\n",
      "[[2599  833]\n",
      " [ 187  381]]\n",
      "Epoch 12/30 | Time: 2.70s\n",
      "  Train Loss: 0.4100\n",
      "  Val Loss: 0.4861 | Val AUC: 0.7170 | Val F1: 0.4388\n",
      "  Val Accuracy: 0.7628 | Val Precision: 0.3304 | Val Recall: 0.6532\n",
      "  Val Confusion Matrix:\n",
      "[[2680  752]\n",
      " [ 197  371]]\n",
      "Epoch 13/30 | Time: 3.02s\n",
      "  Train Loss: 0.4027\n",
      "  Val Loss: 0.4838 | Val AUC: 0.7177 | Val F1: 0.4407\n",
      "  Val Accuracy: 0.7652 | Val Precision: 0.3330 | Val Recall: 0.6514\n",
      "  Val Confusion Matrix:\n",
      "[[2691  741]\n",
      " [ 198  370]]\n",
      "Epoch 14/30 | Time: 2.78s\n",
      "  Train Loss: 0.4059\n",
      "  Val Loss: 0.4811 | Val AUC: 0.7151 | Val F1: 0.4392\n",
      "  Val Accuracy: 0.7670 | Val Precision: 0.3336 | Val Recall: 0.6426\n",
      "  Val Confusion Matrix:\n",
      "[[2703  729]\n",
      " [ 203  365]]\n",
      "Epoch 15/30 | Time: 2.77s\n",
      "  Train Loss: 0.3955\n",
      "  Val Loss: 0.4832 | Val AUC: 0.7154 | Val F1: 0.4391\n",
      "  Val Accuracy: 0.7662 | Val Precision: 0.3330 | Val Recall: 0.6444\n",
      "  Val Confusion Matrix:\n",
      "[[2699  733]\n",
      " [ 202  366]]\n",
      "Epoch 16/30 | Time: 2.79s\n",
      "  Train Loss: 0.3945\n",
      "  Val Loss: 0.4859 | Val AUC: 0.7145 | Val F1: 0.4375\n",
      "  Val Accuracy: 0.7648 | Val Precision: 0.3312 | Val Recall: 0.6444\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 202  366]]\n",
      "Epoch 17/30 | Time: 2.78s\n",
      "  Train Loss: 0.3999\n",
      "  Val Loss: 0.4852 | Val AUC: 0.7139 | Val F1: 0.4371\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3312 | Val Recall: 0.6426\n",
      "  Val Confusion Matrix:\n",
      "[[2695  737]\n",
      " [ 203  365]]\n",
      "Epoch 18/30 | Time: 2.75s\n",
      "  Train Loss: 0.4029\n",
      "  Val Loss: 0.4858 | Val AUC: 0.7147 | Val F1: 0.4378\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3315 | Val Recall: 0.6444\n",
      "  Val Confusion Matrix:\n",
      "[[2694  738]\n",
      " [ 202  366]]\n",
      "Epoch 19/30 | Time: 2.75s\n",
      "  Train Loss: 0.4035\n",
      "  Val Loss: 0.4860 | Val AUC: 0.7145 | Val F1: 0.4375\n",
      "  Val Accuracy: 0.7648 | Val Precision: 0.3312 | Val Recall: 0.6444\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 202  366]]\n",
      "Epoch 20/30 | Time: 3.02s\n",
      "  Train Loss: 0.3814\n",
      "  Val Loss: 0.4863 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 21/30 | Time: 2.96s\n",
      "  Train Loss: 0.4178\n",
      "  Val Loss: 0.4862 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 22/30 | Time: 2.84s\n",
      "  Train Loss: 0.4091\n",
      "  Val Loss: 0.4863 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 23/30 | Time: 2.86s\n",
      "  Train Loss: 0.4032\n",
      "  Val Loss: 0.4862 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 24/30 | Time: 2.88s\n",
      "  Train Loss: 0.3934\n",
      "  Val Loss: 0.4862 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 25/30 | Time: 2.82s\n",
      "  Train Loss: 0.4029\n",
      "  Val Loss: 0.4863 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 26/30 | Time: 2.75s\n",
      "  Train Loss: 0.4132\n",
      "  Val Loss: 0.4863 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 27/30 | Time: 2.78s\n",
      "  Train Loss: 0.3951\n",
      "  Val Loss: 0.4865 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 28/30 | Time: 2.78s\n",
      "  Train Loss: 0.3913\n",
      "  Val Loss: 0.4864 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 29/30 | Time: 2.74s\n",
      "  Train Loss: 0.3994\n",
      "  Val Loss: 0.4864 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "Epoch 30/30 | Time: 2.72s\n",
      "  Train Loss: 0.4088\n",
      "  Val Loss: 0.4864 | Val AUC: 0.7154 | Val F1: 0.4385\n",
      "  Val Accuracy: 0.7650 | Val Precision: 0.3318 | Val Recall: 0.6461\n",
      "  Val Confusion Matrix:\n",
      "[[2693  739]\n",
      " [ 201  367]]\n",
      "\n",
      "--- Training Finished ---\n",
      "Best validation AUC: 0.7255 achieved at epoch 8\n",
      "\n",
      "--- Evaluating on Test Set ---\n",
      "Loaded best model weights for testing.\n",
      "Test Set Performance:\n",
      "-  Test Loss: 0.8716\n",
      "-  Test AUC: 0.6429\n",
      "-  Test F1: 0.3293\n",
      "-  Test Accuracy: 0.4785\n",
      "-  Test Precision: 0.2028\n",
      "-  Test Recall: 0.8752\n",
      "-  Test Confusion Matrix:\n",
      "[[1402 2013]\n",
      " [  73  512]]\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters and Setup ---\n",
    "model_save_path = \"./data/V7_1_best_transformer_model_weighted_sampler_dropout03.pth\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4 # Common starting point for transformers\n",
    "WEIGHT_DECAY = 1e-4 # AdamW uses weight decay\n",
    "EPOCHS = 30 # Start with a reasonable number, monitor validation loss/AUC\n",
    "D_MODEL = 128\n",
    "N_HEAD = 2\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3 # Increased dropout slightly\n",
    "\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = MedicalTimeSeriesDatasetTimeGrid(train_data, y_train)\n",
    "val_dataset = MedicalTimeSeriesDatasetTimeGrid(val_data, y_val)\n",
    "test_dataset = MedicalTimeSeriesDatasetTimeGrid(test_data, y_test)\n",
    "\n",
    "# --- Handle Imbalance (Method 1: Weighted Loss) ---\n",
    "# Calculate weights: weight = total_samples / (num_classes * samples_in_class)\n",
    "# Or simpler for binary: weight for positive class = num_negative / num_positive\n",
    "# num_positives = y_train.sum()\n",
    "# num_negatives = len(y_train) - num_positives\n",
    "# pos_weight_val = num_negatives / num_positives\n",
    "# pos_weight = torch.tensor([pos_weight_val], dtype= torch.float32,  device=DEVICE) # Wrap in tensor for BCEWithLogitsLoss\n",
    "# print(f\"Calculated positive class weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# # Loss Function with weighting\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#---------------------\n",
    "\n",
    "# --- Handle Imbalance (Method 2: Weighted Sampler - Use either this OR weighted loss, or sometimes both) ---\n",
    "# Comment out the WeightedRandomSampler lines if using only weighted loss.\n",
    "# Using both can sometimes be beneficial but start with one. Let's use WeightedRandomSampler here.\n",
    "\n",
    "class_counts = np.bincount(y_train.astype(int))\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = np.array([class_weights[int(t)] for t in y_train])\n",
    "sample_weights = torch.from_numpy(sample_weights).double()\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # Use this if using WeightedRandomSampler\n",
    "# Use the sampler ONLY for the training loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "# ---------------------\n",
    "\n",
    "# For validation and test, use standard sequential loading\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Model, Optimizer ---\n",
    "model = TimeSeriesTransformer(\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3)\n",
    "\n",
    "# --- Training Loop ---\n",
    "best_val_auc = -1.0 # Or use F1 score, depending on priority\n",
    "best_epoch = -1\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_f1': []}\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_epoch_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_metrics = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    val_loss = val_metrics['loss']\n",
    "    val_auc = val_metrics['auc']\n",
    "    val_f1 = val_metrics['f1']\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    end_epoch_time = time.time()\n",
    "    epoch_duration = end_epoch_time - start_epoch_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Time: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_metrics['accuracy']:.4f} | Val Precision: {val_metrics['precision']:.4f} | Val Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"  Val Confusion Matrix:\\n{val_metrics['conf_matrix']}\")\n",
    "\n",
    "\n",
    "    # Optional: Learning rate scheduling step based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the best model based on validation AUC (or F1)\n",
    "    # If using F1, ensure it's calculated correctly (might need probability adjustment/threshold tuning)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"  * Best model saved based on Val AUC: {best_val_auc:.4f} at epoch {epoch+1}\")\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f} achieved at epoch {best_epoch+1}\")\n",
    "\n",
    "# --- Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "print(\"Loaded best model weights for testing.\")\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"-  Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"-  Test AUC: {test_metrics['auc']:.4f}\")\n",
    "print(f\"-  Test F1: {test_metrics['f1']:.4f}\")\n",
    "print(f\"-  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"-  Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"-  Test Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"-  Test Confusion Matrix:\\n{test_metrics['conf_matrix']}\")\n",
    "\n",
    "# --- Optional: Plot training history ---\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history['train_loss'], label='Train Loss')\n",
    "# plt.plot(history['val_loss'], label='Val Loss')\n",
    "# plt.title('Loss History')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history['val_auc'], label='Val AUC')\n",
    "# plt.plot(history['val_f1'], label='Val F1')\n",
    "# plt.title('Validation Metrics History')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Metric Value')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
