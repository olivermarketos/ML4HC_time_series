{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from helper_funcs import get_data_loaders, get_model, train_model, evaluate\n",
    "from Transformers import ProjectionHead\n",
    "plt.style.use('default')\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from Transformers import MedicalTimeSeriesDatasetTuple, collate_fn\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_type\": \"contrast\",  # Choose 'time_grid', 'tuple', \"contrast\", linear_probe\n",
    "    \"scaler_name\": \"MinMaxScaler\", # Name of the scaler used\n",
    "\n",
    "    \"seed\": 42,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\",\n",
    "    \"epochs\": 30, # Adjust as needed\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"patience\": 8, # For early stopping based on validation performance\n",
    "    \"optimizer\": \"adam\", # Choose 'adam', 'sgd', or 'adamw'\n",
    "    \"loss_function\": \"bce\", # Choose 'bce', 'focal', or 'mse'\n",
    "\n",
    "    # Paths (MODIFY THESE)\n",
    "    \"data_dir\": \"data\", # Directory containing train/val/test data\n",
    "    \"output_dir\": \"data/model_outputs\", # Directory to save models and results\n",
    "\n",
    "    # Model Specific Hyperparameters (adjust based on model_type and tuning)\n",
    "    # Grid Transformer\n",
    "    \"grid_d_model\": 128,\n",
    "    \"grid_nhead\": 4,\n",
    "    \"grid_num_layers\": 2,\n",
    "    \"grid_dropout\": 0.2,\n",
    "    \"grid_feature_dim\": 41, # Should match your data\n",
    "\n",
    "    # Tuple Transformer\n",
    "    \"tuple_d_model\": 128,\n",
    "    \"tuple_nhead\": 8,\n",
    "    \"tuple_num_encoder_layers\": 3,\n",
    "    \"tuple_dim_feedforward\": 256, # Typically 2-4x d_model\n",
    "    \"tuple_dropout\": 0.2,\n",
    "    \"tuple_num_modalities\": 41, # 40 variables + 1 for padding (if PAD_INDEX_Z=0) Adjust if needed!\n",
    "    \"PAD_INDEX_Z\": 0, # Padding index for time and value features\n",
    "    \"tuple_modality_emb_dim\": 64,\n",
    "    \"tuple_max_seq_len\": 768, # From transformers.py or defined here\n",
    "\n",
    "    # Contrastive Learning\n",
    "    \"proj_dim\": 128\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Add this near the top of your script ---\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility in PyTorch, NumPy, and Python.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value) # Numpy module\n",
    "    torch.manual_seed(seed_value) # PyTorch CPU seeding\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "        # Configure CuDNN for deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # Optional: Newer PyTorch versions might require this for full determinism\n",
    "        # Note: This can sometimes throw errors if a deterministic implementation isn't available\n",
    "        # try:\n",
    "        #     torch.use_deterministic_algorithms(True)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Warning: Could not enable deterministic algorithms: {e}\")\n",
    "        # Optional: Sometimes needed for deterministic matrix multiplication\n",
    "        # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    print(f\"Seed set globally to {seed_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Call this function very early in your script ---\n",
    "seed = config[\"seed\"]\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(embeddings1, embeddings2, temperature = 0.2):\n",
    "    \"\"\"\n",
    "    embeddings1: Tensor of shape (batch_size, proj_dim) for view 1\n",
    "    embeddings2: Tensor of shape (batch_size, proj_dim) for view 2\n",
    "    \"\"\"\n",
    "    batch_size = embeddings1.shape[0]\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    embeddings1 = nn.functional.normalize(embeddings1, dim=1)\n",
    "    embeddings2 = nn.functional.normalize(embeddings2, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(embeddings1, embeddings2.T) / temperature # shape (batch_size, batch_size)\n",
    "    labels = torch.arange(batch_size, device=embeddings1.device)\n",
    "\n",
    "    loss1 = nn.functional.cross_entropy(similarity_matrix, labels)\n",
    "    loss2 = nn.functional.cross_entropy(similarity_matrix.T, labels)\n",
    "    loss = (loss1 + loss2) / 2.0\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(encoder, projection_head, dataloader, optimizer, device):\n",
    "    encoder.train()\n",
    "    projection_head.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        (t_seq1, z_seq1, v_seq1, attn_mask1), (t_seq2, z_seq2, v_seq2, attn_mask2), _ = batch\n",
    "        t_seq1, z_seq1, v_seq1, attn_mask1 = t_seq1.to(device), z_seq1.to(device), v_seq1.to(device), attn_mask1.to(device)\n",
    "        t_seq2, z_seq2, v_seq2, attn_mask2 = t_seq2.to(device), z_seq2.to(device), v_seq2.to(device), attn_mask2.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        rep1 = encoder.get_representation(t_seq1, z_seq1, v_seq1, attn_mask1)\n",
    "        rep2 = encoder.get_representation(t_seq2, z_seq2, v_seq2, attn_mask2)\n",
    "        proj1 = projection_head(rep1)\n",
    "        proj2 = projection_head(rep2)\n",
    "        loss = info_nce_loss(proj1, proj2, temperature=0.2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loader,_, _ = get_data_loaders(config) \n",
    "\n",
    "\n",
    "\n",
    "device = config[\"device\"]\n",
    "encoder = get_model(config)\n",
    "\n",
    "encoder.to(device)\n",
    "\n",
    "projection_head = ProjectionHead(config[\"tuple_d_model\"], config[\"proj_dim\"])\n",
    "projection_head.to(device)\n",
    "projection_head = ProjectionHead(config[\"tuple_d_model\"], config[\"proj_dim\"])\n",
    "projection_head.to(device)\n",
    "\n",
    "optimizer_pretrain = torch.optim.AdamW(list(encoder.parameters()) + list(projection_head.parameters()), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "    train_loss = train_contrastive(encoder,projection_head, contrastive_loader, optimizer_pretrain, device)\n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the encoder and projection head\n",
    "\n",
    "model_name = f\"V3_contrastive_encoder_model_epoch_{config['epochs']}.pth\"\n",
    "torch.save(encoder.state_dict(), f\"{config['output_dir']}/{model_name}.pth\")\n",
    "torch.save(projection_head.state_dict(), f\"{config['output_dir']}/{model_name}_projection_head.pth\")\n",
    "print(f\"Model saved as {model_name}\")\n",
    "# save config\n",
    "with open(f\"{config['output_dir']}/config_{model_name}.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, model_type, linear_probe=None):\n",
    "    model.eval()\n",
    "    if linear_probe:\n",
    "        linear_probe.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            if model_type == \"tuple\":\n",
    "                t_seq, z_seq, v_seq, attn_mask, labels = batch\n",
    "                t_seq, z_seq, v_seq, attn_mask, labels = t_seq.to(device), z_seq.to(device), v_seq.to(device), attn_mask.to(device), labels.to(device)\n",
    "                logits = model(t_seq, z_seq, v_seq, attn_mask)\n",
    "                y = labels\n",
    "\n",
    "                # Store original labels for metrics\n",
    "\n",
    "\n",
    "            elif model_type == \"time_grid\":\n",
    "                x, labels = batch\n",
    "                x = x.to(device)\n",
    "                y = labels.to(device)\n",
    "                logits = model(x)\n",
    "\n",
    "                # Store original labels for metrics\n",
    "\n",
    "            elif model_type == \"linear_probe\":\n",
    "                t_seq, z_seq, v_seq, attn_mask, labels = batch\n",
    "                t_seq, z_seq, v_seq, attn_mask, labels = t_seq.to(device), z_seq.to(device), v_seq.to(device), attn_mask.to(device), labels.to(device)\n",
    "                rep = model.get_representation(t_seq, z_seq, v_seq, attn_mask)\n",
    "                logits = linear_probe(rep)\n",
    "                y = labels\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "        \n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.extend(probs.flatten().flatten())\n",
    "            all_labels.extend(y.cpu().numpy().flatten())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    auroc = roc_auc_score(all_labels, all_preds)\n",
    "    auprc = average_precision_score(all_labels, all_preds)\n",
    "    confusion_matrix_result = confusion_matrix(all_labels, (all_preds > 0.5).astype(int))\n",
    "    return avg_loss, auroc, auprc, confusion_matrix_result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoder and projection head\n",
    "config[\"model_type\"] = \"contrast\"\n",
    "encoder = get_model(config)\n",
    "encoder.load_state_dict(torch.load(f\"{config['output_dir']}/{model_name}.pth\"))\n",
    "encoder.to(device)\n",
    "projection_head = ProjectionHead(config[\"tuple_d_model\"], config[\"proj_dim\"])\n",
    "projection_head.load_state_dict(torch.load(f\"{config['output_dir']}/{model_name}_projection_head.pth\"))\n",
    "projection_head.to(device)\n",
    "\n",
    "# freeze the encoder\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "linear_probe = nn.Linear(config[\"proj_dim\"], 1) \n",
    "linear_probe.to(device)\n",
    "\n",
    "optimizer_probe = torch.optim.AdamW(linear_probe.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "config[\"model_type\"] = \"linear_probe\" # Set the model type to linear probe for training\n",
    "# Get the data loaders for training, validation, and testing\n",
    "train_loader, val_loader, test_loader = get_data_loaders(config)\n",
    "\n",
    "\n",
    "best_val_auroc = -1.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = os.path.join(config[\"output_dir\"], f\"{model_name}_linear_probe.pth\")\n",
    "\n",
    "print(\"Training Linear Probe...\")\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "    \n",
    "    train_loss = train_model(encoder, train_loader, criterion, optimizer_probe, device, model_type= config[\"model_type\"],linear_probe=linear_probe)\n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_loss, val_auroc, val_auprc, _ = evaluate(encoder, val_loader, criterion, device, model_type=config[\"model_type\"], linear_probe=linear_probe)\n",
    "    if val_auroc > best_val_auroc:\n",
    "            print(f\"Validation AuROC improved ({best_val_auroc:.4f} -> {val_auroc:.4f}). Saving model...\")\n",
    "            best_val_auroc = val_auroc\n",
    "            torch.save(linear_probe.state_dict(), best_model_path)\n",
    "            epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation AuROC did not improve. ({epochs_no_improve}/{config['patience']})\")\n",
    "\n",
    "    if epochs_no_improve >= config[\"patience\"]:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_probe.load_state_dict(torch.load(best_model_path))\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "config[\"model_type\"] = \"linear_probe\" # Set the model type to linear probe for evaluation\n",
    "test_loss, test_auroc, test_auprc, cm = evaluate(encoder, test_loader, criterion, config[\"device\"], config[\"model_type\"], linear_probe=linear_probe)\n",
    "print(\"\\n--- Test Results ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test AuROC: {test_auroc:.4f}\")\n",
    "print(f\"Test AuPRC: {test_auprc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(\"--------------------\")\n",
    "results = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_auroc\": test_auroc,\n",
    "    \"test_auprc\": test_auprc,\n",
    "    \"confusion_matrix\": cm.tolist(),  # Convert to list for JSON serialization\n",
    "}\n",
    "# Save results to a JSON file\n",
    "results_path = os.path.join(config[\"output_dir\"], f\"results_{model_name}_linear_probe.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Simulating sparse labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratified_subset_indices(labels, subset_size, random_state=None):\n",
    "    \"\"\"\n",
    "    Get stratified subset indices from the labels.\n",
    "    \n",
    "    Args:\n",
    "        labels (array-like): The labels for stratification.\n",
    "        subset_size (int): The size of the subset to sample.\n",
    "        random_state (int, optional): Random state for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        list: Indices of the sampled subset.\n",
    "    \"\"\"\n",
    "    dummy_features = np.zeros((len(labels)))  # Dummy features for stratification\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=subset_size, random_state=random_state)\n",
    "\n",
    "    for train_index,_  in sss.split(dummy_features, labels):\n",
    "        return train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "seed = config[\"seed\"]\n",
    "data_path = config[\"data_dir\"]\n",
    "\n",
    "y_train = np.load(os.path.join(data_path, 'outcomes_sorted_set-a.npy'), allow_pickle=True)\n",
    "y_val = np.load(os.path.join(data_path, 'outcomes_sorted_set-b.npy'),allow_pickle=True)\n",
    "y_test = np.load(os.path.join(data_path, 'outcomes_sorted_set-c.npy'), allow_pickle=True)\n",
    "\n",
    "scaler_name = config[\"scaler_name\"]\n",
    "data_format = \"tuple\" # or \"tuple\" depending on your data format\n",
    "X_train = np.load(os.path.join(data_path,f\"{data_format}_processed_{scaler_name}_set-a.npy\"), allow_pickle=True)\n",
    "X_val = np.load(os.path.join(data_path,f\"{data_format}_processed_{scaler_name}_set-b.npy\"), allow_pickle=True)\n",
    "X_test = np.load(os.path.join(data_path,f\"{data_format}_processed_{scaler_name}_set-c.npy\"), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Tuple Transformer on 100, 500 and 1000 subset samples\n",
    "subset_sizes = [\"100\", \"500\", \"1000\"]\n",
    "\n",
    "idx_100 = get_stratified_subset_indices(y_train, 100, random_state=seed)\n",
    "idx_500 = get_stratified_subset_indices(y_train, 500, random_state=seed)\n",
    "idx_1000 = get_stratified_subset_indices(y_train, 1000, random_state=seed)\n",
    "\n",
    "y_train_100 = y_train[idx_100]\n",
    "X_train_100 = X_train[idx_100]\n",
    "\n",
    "y_train_500 = y_train[idx_500]\n",
    "X_train_500 = X_train[idx_500]\n",
    "\n",
    "y_train_1000 = y_train[idx_1000]\n",
    "X_train_1000 = X_train[idx_1000]\n",
    "\n",
    "subsets = {\n",
    "    \"100\": (X_train_100, y_train_100),\n",
    "    \"500\": (X_train_500, y_train_500),\n",
    "    \"1000\": (X_train_1000, y_train_1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MedicalTimeSeriesDatasetTuple(X_val, y_val, max_seq_len=config[\"tuple_max_seq_len\"])\n",
    "test_dataset = MedicalTimeSeriesDatasetTuple(X_test, y_test, max_seq_len=config[\"tuple_max_seq_len\"])\n",
    "collate_func = collate_fn # Use the custom collate_fn\n",
    "\n",
    "train_loaders = {}\n",
    "\n",
    "for size, (X_train_subset, y_train_subset) in subsets.items():\n",
    "    print(f\"Subset size: {size}\")\n",
    "\n",
    "    tmp_train_dataset = MedicalTimeSeriesDatasetTuple(X_train_subset, \n",
    "                                                      y_train_subset, \n",
    "                                                      max_seq_len=config[\"tuple_max_seq_len\"])\n",
    "\n",
    "    class_counts = np.bincount(y_train_subset.astype(int))\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[int(t)] for t in y_train_subset])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(tmp_train_dataset, batch_size=config[\"batch_size\"], sampler=sampler, collate_fn=collate_func)\n",
    "    train_loaders[size] = train_loader\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=collate_func)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=collate_func)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model_type\"] = \"tuple\" # Set the model type to tuple for training\n",
    "device = config[\"device\"]\n",
    "\n",
    "for size, train_loader in train_loaders.items():\n",
    "    print(f\"Training Tuple Transformer on subset size: {size}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = get_model(config)\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=config[\"patience\"])\n",
    "\n",
    "    # 3. Training Loop\n",
    "    best_val_auroc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "\n",
    "    model_name = f\"tuple_transformer_{size}_{config['epochs']}.pth\"\n",
    "    best_model_path = os.path.join(config[\"output_dir\"], f\"{model_name}.pth\")\n",
    "    config_file_path = os.path.join(config[\"output_dir\"], f\"config_{model_name}.json\")\n",
    "    with open(config_file_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, config[\"device\"], config[\"model_type\"])\n",
    "        val_loss, val_auroc, val_auprc, _ = evaluate(model, val_loader, criterion, config[\"device\"], config[\"model_type\"])\n",
    "\n",
    "        scheduler.step(val_auroc)\n",
    "\n",
    "        if val_auroc > best_val_auroc:\n",
    "            print(f\"Validation AuROC improved ({best_val_auroc:.4f} -> {val_auroc:.4f}). Saving model...\")\n",
    "            best_val_auroc = val_auroc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Validation AuROC did not improve. ({epochs_no_improve}/{config['patience']})\")\n",
    "\n",
    "        if epochs_no_improve >= config[\"patience\"]:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "        \n",
    "    # 4. Load Best Model and Evaluate on Test Set\n",
    "    print(\"\\n--- Loading Best Model for Testing ---\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=config[\"device\"]))\n",
    "\n",
    "        print(\"\\n--- Evaluating on Test Set ---\")\n",
    "        test_loss, test_auroc, test_auprc, cm = evaluate(model, test_loader, criterion, config[\"device\"], config[\"model_type\"])\n",
    "        print(\"\\n--- Test Results ---\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test AuROC: {test_auroc:.4f}\")\n",
    "        print(f\"Test AuPRC: {test_auprc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(\"--------------------\")\n",
    "        results = {\n",
    "            \"Test Loss\": test_loss,\n",
    "            \"Test AuROC\": test_auroc,\n",
    "            \"Test AuPRC\": test_auprc,\n",
    "            \"Confusion Matrix\": cm.tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        results_file_path = os.path.join(config[\"output_dir\"], f\"results_{model_name}.json\")\n",
    "        with open(results_file_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "    else:\n",
    "        print(f\"Best model not found at {best_model_path}. Skipping evaluation.\")\n",
    "        \n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test subsets with self-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoder and projection head\n",
    "config[\"model_type\"] = \"contrast\"\n",
    "encoder = get_model(config)\n",
    "encoder_name = f\"V3_contrastive_encoder_model_epoch_30.pth\"\n",
    "encoder.load_state_dict(torch.load(f\"{config['output_dir']}/{encoder_name}.pth\"))\n",
    "encoder.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for size, train_loader in train_loaders.items():\n",
    "    print(f\"Training Linear Probe on subset size: {size}\")\n",
    "\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    config[\"model_type\"] = \"linear_probe\"\n",
    "    linear_probe = nn.Linear(config[\"proj_dim\"], 1) \n",
    "    linear_probe.to(device)\n",
    "\n",
    "    optimizer_probe = torch.optim.AdamW(linear_probe.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    _, val_loader, test_loader = get_data_loaders(config)\n",
    "\n",
    "    best_val_auroc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    probe_name = f\"{encoder_name}_linear_probe_subset_{size}\"\n",
    "    best_model_path = os.path.join(config[\"output_dir\"], f\"{probe_name}.pth\")\n",
    "    config_file_path = os.path.join(config[\"output_dir\"], f\"config_{probe_name}.json\")\n",
    "    with open(config_file_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(\"Training Linear Probe...\")\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        train_loss = train_model(encoder, train_loader, criterion, optimizer_probe, device, model_type= config[\"model_type\"],linear_probe=linear_probe)\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_loss, val_auroc, val_auprc, _ = evaluate(encoder, val_loader, criterion, device, model_type=config[\"model_type\"], linear_probe=linear_probe)\n",
    "        if val_auroc > best_val_auroc:\n",
    "                print(f\"Validation AuROC improved ({best_val_auroc:.4f} -> {val_auroc:.4f}). Saving model...\")\n",
    "                best_val_auroc = val_auroc\n",
    "                torch.save(linear_probe.state_dict(), best_model_path)\n",
    "                epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Validation AuROC did not improve. ({epochs_no_improve}/{config['patience']})\")\n",
    "\n",
    "        if epochs_no_improve >= config[\"patience\"]:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "        \n",
    "\n",
    "    linear_probe.load_state_dict(torch.load(best_model_path))\n",
    "    print(\"\\n--- Evaluating on Test Set ---\")\n",
    "    test_loss, test_auroc, test_auprc, cm = evaluate(encoder, test_loader, criterion, config[\"device\"], config[\"model_type\"], linear_probe=linear_probe)\n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test AuROC: {test_auroc:.4f}\")\n",
    "    print(f\"Test AuPRC: {test_auprc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(\"--------------------\")\n",
    "    results = {\n",
    "        \"Test Loss\": test_loss,\n",
    "        \"Test AuROC\": test_auroc,\n",
    "        \"Test AuPRC\": test_auprc,\n",
    "        \"Confusion Matrix\": cm.tolist()  # Convert to list for JSON serialization\n",
    "    }\n",
    "    results_file_path = os.path.join(config[\"output_dir\"], f\"results_{probe_name}.json\")\n",
    "    with open(results_file_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualising learned representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config[\"model_type\"] = \"tuple\"\n",
    "train_loader, val_loader, test_loader = get_data_loaders(config) \n",
    "\n",
    "config[\"model_type\"] = \"contrast\"\n",
    "\n",
    "encoder = get_model(config)\n",
    "encoder.load_state_dict(torch.load(f\"{config['output_dir']}/V3_contrastive_encoder_model_epoch_30.pth.pth\"))\n",
    "encoder.to(device)\n",
    "\n",
    "encoder.eval()\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Generating embeddings\", leave=False):\n",
    "        t_seq, z_seq, v_seq, attn_mask, label = batch\n",
    "        t_seq, z_seq, v_seq, attn_mask = t_seq.to(device), z_seq.to(device), v_seq.to(device), attn_mask.to(device)\n",
    "        rep = encoder.get_representation(t_seq, z_seq, v_seq, attn_mask)\n",
    "        embeddings.append(rep.cpu().numpy())\n",
    "        labels.append(label.cpu().numpy())\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "labels = np.concatenate(labels, axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_embeddings = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Label (e.g., Death=1, Survival=0)')\n",
    "plt.title(\"t-SNE Visualization of Patient Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Label (e.g., Death=1, Survival=0)')\n",
    "plt.title(\"t-SNE Visualization of Patient Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# embeddings_2d: your 2D t-SNE or UMAP embeddings\n",
    "# labels: corresponding class labels for each data point\n",
    "sil_score = silhouette_score(embeddings, labels)\n",
    "print(\"Silhouette Score:\", sil_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
